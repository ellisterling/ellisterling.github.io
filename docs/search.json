[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/enso-predictor/index.html",
    "href": "posts/enso-predictor/index.html",
    "title": "Attempting to Predict ENSO Phases",
    "section": "",
    "text": "Abstract\nModeling the El Niño Southern Oscillation is a problem that has plagued climate scientists due to the lack of observed data to train models on. This blog post will describe how I attempted to train a Logistic Regression model on observed and reconstructed sea surface temperature data in order to predict ENSO phases. The data processing for this project was what took the most time, and had I been more proficient with using spatiotemporal data, I may have had more time to implement more complex algorithms and find more success. I used sea surface temperature (SST) data from the Pacific Ocean, clipped to exclude extra cold areas and other oceans. I then split that data into regions, which I used as features. In the end, my Logistic Regression models using both Gradient Descent and Adam optimizers ended up achieving 50-60% accuracy on training data. I then input my feature matrix and target vector into five scikit-learn models, which found slightly more success. I then posited on future steps and modeling techniques that could make this project successful in the future.\nThe source code for this project is located here.\n\n\nIntroduction\nThe El Niño Southern Oscillation (ENSO) is a climate circulation pattern resulting from temperature dynamics in the Equatorial Pacific Ocean. Colder temperatures along the South American coast indicate a La Niña year, and can cause increased rainfall in areas like India with a pronounced monsoon season. Figure 1 illustrates what the sea surface temperatures can look like in the Pacific when a La Niña phase is happening.\n\n\n\nFigure 1: an example of plotted SST from a La Niña phase (September 1, 2008).\n\n\nENSO is a phenomenon that is interconnected with the entire earth system. The figure below from Anderson and Lucas (2008) illustrates some of the Earth System processes that affect and are affected by ENSO. Specifically, it illustrates how wind and precipitation are broadly affected by (and how they affect) the SST in different parts of the Pacific.\n\n\n\nFigure 2: A conceptual diagram of ENSO and some of its interconnectivity with atmospheric systems sourced from Anderson and Lucas (2008). Figure (a) shows La Niña. Figure (b) shows El Niño.\n\n\nIn doing this project, I wanted to see if simple Logistic Regression could classify future El Niño stages 6 months in advance based on current sea surface temperature variables. Others have implemented machine learning algorithms to solve this problem with climate model output integration and/or increased features in terms of factors indicating ENSO (SST, sea surface height, precipitation, etc.). Specifically, it is difficult to create a model that doesn’t include any climate modeling because satellite monitoring of Earth only started in the late 1900s (Chen, Gilani, and Harlim (2021)), so there is an extremely limited range of data available to train models on. Most models, even if they don’t use predictive models to directly predict ENSO conditions, will use reconstructed climate from a Global Climate Model in order to expand their dataset. Chen, Gilani, and Harlim (2021) implemented a Bayesian model that avoided the use of modeling that was able to achieve better accuracy than some models that are integrated with climate models. Notably, this model included many more features than we will use in this implementation. Hernández, Mesa, and Lall (2020) were also able to successfully predict ENSO using a non-homogenous Hidden Markov Model. After learning about HMMs in AI, this implementation makes a lot of sense based on how climate proxies are also, in a way, indicators of hidden states. In this blog post, I will describe how I attempted to implement a Logistic Regression model able to predict ENSO stages with reduced features from other implementations.\n\n\nValues Statement\nThe potential users of this project, had it been more successful, would have been policymakers and communities who are generally more affected by El Niño/La Niña conditions. Additionally, scientists could use this model and improve it using climate modeling. Other than these users, people who could be affected by this project include people who live in areas that are heavily impacted by these events–a wrong prediction could be hazardous if communities prepare in one way and are unprepared for another. I think these are the same people who would benefit from this technology. Additionally, it would in theory be less computationally expensive than implementing a climate model and using those results to inform predictions, so it would have been helpful to scientists and others implementing climate prediction models had it worked. People who could be harmed include the previously mentioned groups since a wrong prediction could be detrimental to communities preparing for one outcome and getting another.\nI wanted to work on this problem because, after reading about ENSO and learning about it in classes, I was interested to see if sea surface temperature (SST) alone (initially, along with precipitation) could predict future stages. I had learned a lot about climate modeling in some of my Earth and Climate Science classes, and wanted to see if I could implement algorithms from this class to predict ENSO events. I also just wanted to combine my ECSC knowledge with a project for CS!\nI do not think the world will be a necessarily more or less equitable, just, joyful, or sustainable place because of what I implented, mainly because it was not very successful in terms of what I was able to produce. I think that the main effect this technology will have will be on me, simply because now I know firsthand how it is very difficult to predict weather patterns, even well-defined ones.\n\n\nMaterials and Methods\n\nData\nFor the SST data, I used the Japan Meteorological Associaation’s COBE-SST 2 dataset (Japanese Meteorological Association (2025)). COBE-SST 2 and Sea Ice data was provided by the NOAA PSL, Boulder, Colorado, USA, from their website at https://psl.noaa.gov. This dataset goes back to 1850, but I had to subset this data to only include years after 1950. This was necessary in order to match the range of the feature matrix with the data I used for the target vector. Much of this data has been filled in/reconstructed in order to account for the lack of data before the late 1900s. The target data was derived from the Oceanic Niño Index (ONI) dataset, sourced from the National Weather Service Climate Prediction Center (2013). The ONI goes back to 1950, and represents an ocean-wide anomaly (or change from the normal). An ONI of &lt; -0.5 indicates a La Niña phase, whereas an ONI of &gt; 0.5 indicates an El Niño phase. Each row in my dataset represents the yearly SST average. The columns of my feature matrix are all regions of the ocean. There is no precipitation data included in this dataset, which I will explain in the following section.\n\n\nApproach\nThe first thing I did in order to process this data was to clip it to only include data from the Pacific Ocean. I also clipped out the ocean closest to the North and South Poles because their temperature is always hovering around 32 degrees Fahrenheit regardless of the ENSO phase, and ENSO is mainly affected by just the Equatorial Pacific.\nI went through this data and turned it into first ternary data and then binary. This was because I did not have time to implement a multinomial optimizer. In the end, I classified years where the ONI was &lt; 0 as La Niña, and years where the ONI was &gt; 0 as El Niño. This simplification certainly could have caused issues, but when I ran an sklearn model on each of them, I found that the target vector with three classes actually resulted in worse accuracy (probably because I didn’t have enough data points). I used all the regions/features as predictors, which also could have affected accuracy (again, because of the small amount of data points I had.)\nIn the time given to work on the project I couldn’t figure out how to deal with both spatial and temporal data without making the spatial aspect the features. With this structure of the feature matrix, I couldn’t figure out how to add precipitation without changing the whole structure of the data. This means that the only climate data I could use as a feature was SST. I came into this project also wanting to include precipitation, but did not end up getting there with this project.\nWhen I realized how much trouble the data was giving me, I pivoted to just trying to classify current ENSO states, which proved harder than I expected. To do this I used Logistic Regression with Gradient Descent Optimizer and Adam Optimizer. I wasn’t able to get them to reach good accuracy on the training data, so I then pivoted to testing other sklearn models. I created a 60/40 train-test split before running the sklearn implementations of Logistic Regression with LBFGS, liblinear, and Stochastic Gradient descent optimizers. I then ran the sklearn implementations of Naive Bayes and Random Forest classifiers. I evaluated their performance by their accuracy and whether or not they just predicted all 1s or all 0s.\n\n\n\nResults\nWhen I ran the models that I implemented myself, I couldn’t get them to predict anything other than all 1s or all 0s. Depending on the train-test split, sometimes Adam would predict mostly 1s and a few 0s or mostly 0s and a few 1s, but it didn’t affect the accuracy. The accuracy on the training data for both hovered around 50%, which makes sense when you consider how they were only predicting one class for the whole dataset. The loss plots for both of them looked odd, and although Gradient Descent appeared to converge, it converged to a low accuracy.\n\n\n\nFigure 3: Loss over iterations for Adam and Gradient Descent optimizers for Logistic Regression.\n\n\nAfter wrestling with the handmade models, trying to get them to work and failing, I decided to check the sklearn implementations of a few different models to see how they fared. Below are the results of the end accuracy for five different models for one of the runs. Most of the models hover around 50% accuracy, just as our handmade models did. Logistic Regression actually performed the best and most consistently, generally producing accuracies of about 70%-80% with the liblinear and LBFGS optimizers. Occasionally, Naive Bayes would also perform well, but it was more unreliable than the LR implementations.\n\n\n\nFigure 4: Accuracty of five models from the scikit-learn library.\n\n\nI also tested the sklearn models on my data with multinomial classifiers (La Niña, El Niño, or Neutral phases). Interestingly, many of the models generally performed worse when trying to classify the data this way.\n\n\n\nFigure 5: Model accuracy plotted for multinomial and binary classification. The accuracy for the Binary classification is slightly different than in Figure 4 because it came from a different model run with different train-test splits/different starting weight vectors.\n\n\nMultinomial classification performed slightly better for the Bayes model and the SGD model on this run, but it is very close for the Bayes. In many other runs, Binary performed better for Bayes as well, and the only model that consistently performed better with the multinomial data was Stochastic Gradient Descent (SGD). This model also consistently performed the worst of all the models, regardless of target vector.\n\n\nDiscussion\nThe results of this project clearly do not show much success–I wasn’t able to achieve any more than 55% accuracy with the algorithms I implemented, and even most of scikit-learn’s algorithms didn’t have great accuracy. liblinear and LBFGS were two Logistic Regression optimizers that performed very well (70%-80% accuracy), so if I had more time on this project, I would have attempted to implement them. However, the greater question is whether or not this approach was the right one at all. First of all, I used binary classification, which is not accurate to the actual phases, and necessarily classifies neutral phases as El Niño or La Niña. As we saw from the scikit-learn models, implementing multinomial classification probably would have harmed my models’ accuracy. This likely occurred because I didn’t have enough data points. Also, many previous studies attempting to implement machine learning to predict ENSO have done so by combining with predictive climate model output. When I started this project, I was curious if it would be possible to implement a predictor without doing so. Although people have implemented machine learning algorithms that predict ENSO without using future climate model predictions, these algorithms all used many more features than I included. Additionally, I went into this with very little experience doing machine learning with data that was both spatial and temporal, and so my solution to this problem (to assign regions within the Equatorial Pacific and use those regions as features) was maybe not the best approach either. Given that researchers such as Chen, Gilani, and Harlim (2021), Hernández, Mesa, and Lall (2020), and Pal et al. (2020) have been able to achieve very good accuracy with their models, and experts in the field can predict stages without the help of machine learning, I am certain that the general idea of this project has a future in the machine learning and climate science communities. However, the way I went about implementing my algorithm was not the optimal one. Future implementations should either integrate with climate models or include more features to account for the interconnectedness of the climate system. As the climate warms, integration with models may become more necessary because our understanding of ENSO could shift with a warming climate and more intense phases.\n\n\nGroup Contribution\nBecause I was working on this project alone, I worked on all parts of the source code and completed all the work related to this project.\n\n\nPersonal Reflection\nOne thing I learned from the process of creating this project is that things that seem simple can often be quite complicated. I really struggled with the data wrangling–since I had experience using other peoples’ climate models, I assumed that working with spatial data would be fine. However, the combination of spatial and temporal data ended up throwing me for a loop. I think the solution I came up with was reasonable, but it was definitely a “make-do” kind of situation. Additionally, I learned a lot from reading papers about ENSO and how connected it is to the entire Earth system. I had an idea of this from previous classes, but it was cool to be able to read scientific papers on it and see how predictions are made.\nUnfortunately, I did not meet my goal for this project. I thought that a good backup would be to try and classify current ENSO stages instead of predicting them, but I also did not obtain good accuracy for this. However, I do think that I learned a lot about why what I was going to attempt wouldn’t work, and why climate modeling is likely needed in order to confidently predict future atmospheric circulation. Additionally, when I undertook this project, I was aiming for accuracy of 50% or more–which I did obtain. Unfortunately, that accuracy is not enough to be useful in its function as a predictor.\nI had to do a lot of pivoting in this project, and I think that in the future, I will take the flexibility that I gained here and put it to use. It was good practice in scaling down my expectations in the moment and working with what I had. This will be useful in all aspects of my future life, including personal, professional, and any academic work I may continue doing in the future.\n\n\n\n\n\nReferences\n\nAnderson, T. R., and M. I. Lucas. 2008. “Upwelling Ecosystems.” In Encyclopedia of Ecology (Second Edition), edited by Brian Fath, 700–710. Oxford: Elsevier. https://doi.org/10.1016/B978-0-444-63768-0.00363-2.\n\n\nChen, Nan, Faheem Gilani, and John Harlim. 2021. “A Bayesian Machine Learning Algorithm for Predicting ENSO Using Short Observational Time Series.” Geophysical Research Letters 48 (17): e2021GL093704. https://doi.org/10.1029/2021GL093704.\n\n\nHernández, Julián David Rojo, Óscar José Mesa, and Upmanu Lall. 2020. “ENSO Dynamics, Trends, and Prediction Using Machine Learning,” October. https://doi.org/10.1175/WAF-D-20-0031.1.\n\n\nJapanese Meteorological Association. 2025. “COBE-SST 2 and Sea Ice.” Dataset; National Oceanic and Atmospheric Administration. https://psl.noaa.gov/data/gridded/data.cobe2.html.\n\n\nNational Weather Service Climate Prediction Center. 2013. “Description of Changes to the Oceanic Nino Index (ONI).” Dataset; National Oceanic and Atmospheric Administration. https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_change.shtml.\n\n\nPal, Manali, Rajib Maity, J. V. Ratnam, Masami Nonaka, and Swadhin K. Behera. 2020. “Long-Lead Prediction of ENSO Modoki Index Using Machine Learning Algorithms.” Scientific Reports 10 (1): 365. https://doi.org/10.1038/s41598-019-57183-3."
  },
  {
    "objectID": "posts/quantifying-bias/index.html",
    "href": "posts/quantifying-bias/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "Narayanan’s Position\nAs a quantitative researcher, Narayanan acknowledges in his speech that, although he is pointing out the flaws of quantitative methodology, he recognizes its importance in the world of research. Throughout his speech, he highlights several flaws in quantitating discrimination, most of which boil down to the fact that quantitative data does not tell the whole story. His perspective is that quanititative methods are just one piece of the puzzle. They are necessary, but they are not the only way to gain knowledge. Quantitative ways of measuring bias should be used in conjunction with other forms of measurement and auditing to ensure that all perspectives are observed and the fullest picture of the situation is found (Narayanan (2022)).\n\n\nBenefits of Quantifying Bias\nIn Narayanan (2022), he discusses how the ProPublica investigation into the COMPAS algorithm was successful in part because there as a quantifiable result that could “definitively” say whether or not the COMPAS algorithm was biased. Evidently, this succeeded, in part because they were able to quantify ways in which they could see evidence of bias. It then gained a lot of attention because of its success. Narayanan posits that if they had been searching in a more qualitative way, they would have gained less attention from their study and therefore would have reached a smaller audience. Essentially, humans are more likely to believe something if there is evidence for it, and quantitative evidence is sometimes prioritized over qualitative evidence (especially in the scientific community).\n\n\nDrawbacks of Quantifying Bias\nAs outlined by Narayanan:\n• Use of snapshot datasets\n• Use of data provided by company\n• Explaining away discrimination\n• Wrong locus of intervention\n• Objectivity illusion\n• Performativity\nNarayanan outlines several potential pitfalls of quantifying bias in his speech (Narayanan (2022)). The first one he outlines is the choice of null hypothesis. What reserachers choose as their “null hypothesis”, or base assumption of neutrality, drastically affects whether or not we call something biased. Generally, researchers say that the null hypothesis is the absence of discrimination. This sets up studies to be biased from the start, since assuming that the base state is a lack of bias ignores the systemic bias that is present in many aspects of society. Narayanan brings up the example of diversity initiatives in schools and other programs–they focus on fixing the surface level problem without addressing the root cause (discrimination) because the base assumption is that academia is an unbiased field to begin with. He also discusses the bias that is inherently present in data, even more so when the data is provided by an institution with stakes in the investigation. This reminded me of how some fossil fuel companies have funded climate science so that they could control what results came out of the studies.\n\n\nDiscussion and Synthesis\nI generally agree with Narayanan’s perspectives on quantitative methods. I think that they have their place, but also I think what is needed is a perspective shift–that quantitative methods are not always better/more informative than qualitative ones, and that we don’t have to be able to simulate everything perfectly mathematically. While I recognize the benefits of quantitativeness, I also believe that quantifying everything we see will usually create more problems than not.\n\n\n\n\n\n\n\n\nReferences\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n60\n15.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n1\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n18.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n20\n18.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n33\n18.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(99419, 15)\n(99419,)\n(99419,)\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7863608931804466\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7875706214689265\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n\n0.7777164920022063\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n23.0\n21.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n1\n24.0\n18.0\n5.0\n2.0\n2.0\n0.0\n1.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2\nTrue\n\n\n2\n33.0\n13.0\n3.0\n13.0\n1.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1\nTrue\n\n\n3\n88.0\n19.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n4\n4.0\n2.0\n5.0\n2.0\n2.0\n5.0\n1.0\n3.0\n0.0\n4.0\n1.0\n2.0\n2.0\n0.0\n2.0\n1\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n79530\n50.0\n21.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n79531\n70.0\n16.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79532\n18.0\n16.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79533\n78.0\n16.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79534\n22.0\n16.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n\n\n79535 rows × 17 columns\n\n\n\nHow many individuals are in the dataset?\n\nlen(df)\n\n79535\n\n\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1? ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍In ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employment ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍prediction, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍would ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍correspond ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals.\n\nprint(df[\"label\"].value_counts())\nprint(35232/(35232+44303))\n\nlabel\nFalse    44303\nTrue     35232\nName: count, dtype: int64\n0.44297479097252784\n\n\n44.3%\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍how ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍many ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍groups?\n\ndf[\"group\"].value_counts()\n\ngroup\n1    67415\n2     6881\n6     2061\n9     1922\n8      670\n3      467\n5       95\n7       24\nName: count, dtype: int64\n\n\nIn ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1?\n\n(df[df[\"label\"] == True][\"group\"].value_counts())/df[\"group\"].value_counts()\n\ngroup\n1    0.454825\n2    0.345880\n6    0.492479\n9    0.328824\n8    0.444776\n3    0.419700\n5    0.400000\n7    0.458333\nName: count, dtype: float64\n\n\nCheck ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍intersectional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍trends ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍studying ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍broken ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍out ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chosen ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍an ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍additional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍For ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍example, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍if ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(RAC1P) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍then ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍could ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍choose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(SEX) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍compute ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍both ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍might ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍good ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍opportunity ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍use ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍visualization ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍such ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bar ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chart, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍e.g. via ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍seaborn package.\n\n(df[df[\"label\"] == True].groupby(\"group\")[\"SEX\"].value_counts())/df.groupby(\"group\")[\"SEX\"].value_counts()\n\ngroup  SEX\n1      1.0    0.486367\n       2.0    0.423627\n2      1.0    0.308653\n       2.0    0.379828\n3      1.0    0.434389\n       2.0    0.406504\n5      1.0    0.360000\n       2.0    0.444444\n6      1.0    0.573034\n       2.0    0.419593\n7      1.0    0.400000\n       2.0    0.500000\n8      1.0    0.495575\n       2.0    0.392749\n9      1.0    0.323656\n       2.0    0.333669\nName: count, dtype: float64\n\n\nNext step is to make a plot where the x axis is race, y is percentage positive labels, and then the hue of the bar would be indicative of sex. From the pure numbers, we can see that group 2, which represents African Americans in the dataset, is much lower than many of the other groups.\n\nimport seaborn as sns\nsns.barplot(x = \"group\", y = , data = df, hue = \"SEX\")\n\n\n  Cell In[21], line 2\n    sns.barplot(x = \"group\", y = , data = df, hue = \"SEX\")\n                                 ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nThe code for my LR implementation can be found on GitHub here.\nIn this blog post, we will implement Logistic Regression and run various experiments to test our implementation. Logistic Regression uses the gradient of a feature matrix to approximate the best next option for the weight vector w. We iterate through this step until the weight vector is optimal and the loss function has converged. Our initial tests will include a basic test of performance on synthetic data, a comparison of the Gradient Descent Optimizer with and without momentum, and a test in which we force overfitting of the model by increasing the features so they outnumber the number of data points. After these synthetic data experiments, we will run our model on some real-world data to see how it performs on an actual dataset.\n\n\nExperiments\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import GradientDescentOptimizer\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport pandas as pd\nimport numpy as np\n\nFirst, lets define the function with which we will create our synthetic data. The code for this function was taken from Prof. Phil Chodrow’s lecture notes.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\nVanilla Gradient Descent\nNow, we will run our model on this synthetic data with beta=0, which is called Vanilla Gradient Descent. We want to see how it performs without momentum so we can compare later on.\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nfor _ in range(500):\n    loss = opt.loss(X, y)\n    loss_arr.append(loss)\n    opt.step(X, y, alpha = 0.3, beta = 0.0)\n\n\nplt.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"black\")\nplt.title(\"Loss over iterations for Vanilla Gradient Descent\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, we can see that our loss function has converged. Let’s plot our weight vector and see how it separates our data.\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, opt.w)\n\n\n\n\n\n\n\n\nLooking good! Our vector looks visually correct. Now let’s compare with a model where we add momentum.\n\n\nGradient Descent with Momentum\nNow, we want to prove that adding momentum can speed up convergence in some cases. Let’s create a new model with beta = 0.9 and run it for the same number of iterations that we ran our Vanilla Gradient Descent Model for.\n\nopt2 = GradientDescentOptimizer()\nloss_arr2 = []\nfor _ in range(500):\n    loss = opt2.loss(X, y)\n    loss_arr2.append(loss)\n    opt2.step(X, y, alpha = 0.3, beta = 0.9)\n\nNow we can plot the loss curves!\n\nax = plt.axes()\nax.plot(torch.arange(1, len(loss_arr2)+1), loss_arr2, color = \"black\", label = \"GD with Momentum\")\nax.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"blue\", label = \"Vanilla GD\")\nax.set_title(\"Loss for Gradient Descent with Momentum vs Vanilla Gradient Descent\")\nax.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nWe can see that the model with momentum converges much faster than the model without for these choices in alpha and beta.\n\n\nForcing Overfitting\nNow that we’ve confirmed that our model performs as we would expect, let’s test it on some data with too many features. We want to force overfitting in this case.\n\nX_train, y_train = classification_data(n_points=50, noise = 0.5, p_dims=5000)\nX_test, y_test = classification_data(n_points=50, noise = 0.5, p_dims=5000)\n\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nfor _ in range(500):\n    opt.step(X_train, y_train, alpha = 0.3, beta = 0)\n\nWe’ve ran our model, so let’s take a look at the difference between training data accuracy and testing data accuracy.\n\npredicted = opt.predict(X_train)\n(predicted == y_train).sum()/len(predicted)\n\ntensor(1.)\n\n\nWe have 100% accuracy on the training data.\n\npredicted_test = opt.predict(X_test)\n(predicted_test == y_test).sum()/len(predicted_test)\n\ntensor(0.7600)\n\n\nBut only 76% accuracy on the testing data! Because of the amount of features that we put in our dataset, the model tried to hard to be accuracte for all of them, and when we gave it another dataset the accuracy dropped dramatically. Moral of the story: don’t have more features than data points!\n\n\nEmpirical Data\nNow for the fun part! We are going to load in a real-world binary classification dataset and test our model performance on it.\nThe data we will be using is a dataset that contains information related to whether or not people have anemia based on the color of the inside of their eyelid as well as their hemoglobin levels. I obtained this data from the Kaggle. It was collected by Chittagong and Cox’s Bazar Medical College Hospital in Bangladesh. The %red, %blue, and %green fields represent the average pixel percentages from images of patients conjunctiva (the inside of their eyelid).\nFirst, let’s process the data and split it into training, testing, and validation datasets.\n\ndf = pd.read_csv(\"./data/anemia_dataset.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nNumber\nName\n%Red Pixel\n%Green pixel\n%Blue pixel\nHb\nAnaemic\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\n\n\n\n\n0\n1\nJafor Alam\n43.2555\n30.8421\n25.9025\n6.3\nYes\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2\nkhadiza\n45.6033\n28.1900\n26.2067\n13.5\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n3\nLalu\n45.0107\n28.9677\n26.0215\n11.7\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n4\nMira nath\n44.5398\n28.9899\n26.4703\n13.5\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n5\nmonoara\n43.2870\n30.6972\n26.0158\n12.4\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe’ll use the features identified by Patricio et al. as the most pertinent features: Age, BMI, Glucose, and Resistin.\n\ndf = df.drop([\"Number\", \"Name\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\"], axis=1)\ndf\n\n\n\n\n\n\n\n\n%Red Pixel\n%Green pixel\n%Blue pixel\nHb\nAnaemic\n\n\n\n\n0\n43.2555\n30.8421\n25.9025\n6.3\nYes\n\n\n1\n45.6033\n28.1900\n26.2067\n13.5\nNo\n\n\n2\n45.0107\n28.9677\n26.0215\n11.7\nNo\n\n\n3\n44.5398\n28.9899\n26.4703\n13.5\nNo\n\n\n4\n43.2870\n30.6972\n26.0158\n12.4\nNo\n\n\n...\n...\n...\n...\n...\n...\n\n\n99\n49.9999\n29.2860\n20.7141\n14.5\nYes\n\n\n100\n42.2324\n30.6757\n27.0919\n6.3\nYes\n\n\n101\n45.6064\n31.9084\n22.4852\n12.7\nNo\n\n\n102\n45.2095\n29.2769\n25.5136\n13.4\nNo\n\n\n103\n43.5706\n29.8094\n26.6199\n12.2\nNo\n\n\n\n\n104 rows × 5 columns\n\n\n\n\nX = df.drop(\"Anaemic\", axis=1)\ny = df[\"Anaemic\"]\ny = y.replace({\"Yes\": 1, \"No\": 0})\n\n/var/folders/sy/9tmrg3gx65vf4qjw8jl8ytsc0000gn/T/ipykernel_98315/975647156.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  y = y.replace({\"Yes\": 1, \"No\": 0})\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=1) # 25% of 80 is 20, so this should create a 60/20/20 split\n\n\nX_train = torch.from_numpy(X_train.to_numpy(dtype = np.float32))\nX_test = torch.from_numpy(X_test.to_numpy(dtype = np.float32))\nX_val = torch.from_numpy(X_val.to_numpy(dtype = np.float32))\ny_train = torch.tensor(y_train.values, dtype=torch.float32)\ny_test = torch.tensor(y_test.values, dtype=torch.float32)\ny_val = torch.tensor(y_val.values, dtype=torch.float32)\n\nFirst, we’ll run our model with Vanilla Gradient Descent.\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nloss_arr_val = []\nfor _ in range(100):\n    loss = opt.loss(X_train, y_train)\n    loss_val = opt.loss(X_val, y_val)\n    loss_arr.append(loss)\n    loss_arr_val.append(loss_val)\n    opt.step(X_train, y_train, alpha = 0.001, beta = 0)\n\n\nax = plt.axes()\nax.plot(loss_arr, label=\"Training\")\nax.plot(loss_arr_val, label=\"Validation\")\nax.legend()\nax.set_title(\"Loss over Iterations for Training and Validation Data\")\n\nText(0.5, 1.0, 'Loss over Iterations for Training and Validation Data')\n\n\n\n\n\n\n\n\n\nWe can see that it converged, but interestingly the validation loss converged to a slightly smaller value. Now, let’s try to run it with momentum.\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nloss_arr_val = []\nfor _ in range(100):\n    loss = opt.loss(X_train, y_train)\n    loss_val = opt.loss(X_val, y_val)\n    loss_arr.append(loss)\n    loss_arr_val.append(loss_val)\n    opt.step(X_train, y_train, alpha = 0.002, beta = 0.9)\n\n\nax = plt.axes()\nax.plot(loss_arr, label=\"Training\")\nax.plot(loss_arr_val, label=\"Validation\")\nax.legend()\nax.set_title(\"Loss over Iterations for Training and Validation Data with Momentum\")\n\nText(0.5, 1.0, 'Loss over Iterations for Training and Validation Data with Momentum')\n\n\n\n\n\n\n\n\n\nIt also converged! Now let’s check our model’s loss and accuracy on the testing data.\n\nopt.loss(X_test, y_test)\n\ntensor(0.1397)\n\n\n\npredicted = opt.predict(X_test)\n(predicted == y_test).sum()/len(predicted)\n\ntensor(1.)\n\n\nWe got it to converge to a very small loss, and had 100% accuracy on the testing data. This is interesting because the loss curve for the optimizer with momentum is so unexpected–it has a lot of variability and is not always decreasing. However, we were able to reach really good accuracy with this model, and our loss curves did reach convergence.\n\n\n\nDiscussion\nIn this blog post, we implemented Logistic Regression and performed various experiments with both synthetic and real-world data. We were able to achieve 100% accuracy on both types of data, and we pushed the model to its limits by forcing overfitting. We were able to see the difference between loss curves on synthetic and real data, and we were able to see where Logistic Regression with Gradient Descent fails as a model. In general, it seems like it performs better on datasets with not a lot of features. We were able to observe how our model dramatically decreased in accuracy when we gave it more features than data points. One interesting result from our empirical data experiment is that the Vanilla Gradient Descent model converged faster than the model with momentum, but we were able to achieve faster convergence with momentum on the synthetic data. This helps to illustrate how different alpha and beta parameters are going to work better for different datasets."
  },
  {
    "objectID": "posts/ADS/index.html",
    "href": "posts/ADS/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this blog post, we will walk through the process of training a linear regression model, creating a scoring function, and finding an optimal threshold for a scoring and classification problem. The dataset we are working with is one with data about prospective loan borrowers from a bank. It includes various characteristics of each person and the loan they are requesting. We will use this data to determine a threshold and scoring function to determine whether or not, if given the loan, the borrower is likely to default (or, in laymans terms, violate the terms of the loan by not paying/causing the bank to lose money). We will use the scikit-learn library again for this blog post. First, we will train a logistic regression model. Then we will find optimal values for the weight vector and the threshold so that we can determine the optimal profit/borrower.\n\n\nPart A: Grab the Data\nTo begin, let’s import the data:\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nWe can take a look at the columns so that we know what attributes we are working with.\n\ndf_train.columns\n\nIndex(['person_age', 'person_income', 'person_home_ownership',\n       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n       'loan_int_rate', 'loan_status', 'loan_percent_income',\n       'cb_person_default_on_file', 'cb_person_cred_hist_length'],\n      dtype='object')\n\n\n\n\nPart B: Explore The Data\nNow, we can make a few figures to explore what the data is saying. First, I created a scatterplot to show the relationship between a person’s age and their credit history length. As can be inferred, older people generally have longer credit histories.\n\nfig, ax = plt.subplots(1, 1)\n# cut out the huge outlier--person making 6m/yr--so that we can see patterns more clearly\np1 = sns.scatterplot(df_train[df_train[\"person_age\"] &lt; 100], x = \"cb_person_cred_hist_length\", y = 'person_age', hue = 'cb_person_default_on_file', style = 'cb_person_default_on_file')\n\n\n\n\n\n\n\n\nHowever, I was very interested in the fact that whether or not people defaulted on their loan seemed not to be reliant on age. There are orange x’s in all parts of the plot. Because of this, I wanted to look at the mean age for people who had a history of defaulting and people who didn’t.\n\ndf_train.groupby(\"cb_person_default_on_file\")[\"person_age\"].mean()\n\ncb_person_default_on_file\nN    27.721878\nY    27.793096\nName: person_age, dtype: float64\n\n\nAs you can see, although the average age for people who had defaulted was slightly bigger, they still both had averages of about 27. This was surprising to me because I had previously assumed that people who are older, and therefore have longer credit histories, would be more likely to have defaulted on a loan. I wonder if this is due to financial differences between generations.\nNext, I wanted to look at interest rate based on history of defaulting. I assumed before making the plot that people who had defaulted before would be given a higher interest rate, and I was correct.\n\nbarplot = sns.barplot(df_train, x = \"cb_person_default_on_file\", y = \"loan_int_rate\")\nbarplot.set_title(\"Average loan interest rate separated by previous loan defaults\")\n\nText(0.5, 1.0, 'Average loan interest rate separated by previous loan defaults')\n\n\n\n\n\n\n\n\n\nHere, we can see that the average loan interest rate for those who had not defaulted on a loan previously is about 10%. For those who had, it was about 15%. A 5% increase is quite significant, and it seems like having a clean loan history would significantly benefit someone’s chances at getting a good interest rate.\nNext, I wanted to look at the factors which played into loan intent. To do this, I created a table showing the home ownership counts per loan intent.\n\ndf_train.groupby(\"loan_intent\")[\"person_home_ownership\"].value_counts()\n\nloan_intent        person_home_ownership\nDEBTCONSOLIDATION  RENT                     2260\n                   MORTGAGE                 1841\n                   OWN                        62\n                   OTHER                      15\nEDUCATION          RENT                     2612\n                   MORTGAGE                 2089\n                   OWN                       412\n                   OTHER                      14\nHOMEIMPROVEMENT    MORTGAGE                 1384\n                   RENT                     1252\n                   OWN                       255\n                   OTHER                      11\nMEDICAL            RENT                     2740\n                   MORTGAGE                 1730\n                   OWN                       352\n                   OTHER                      13\nPERSONAL           RENT                     2171\n                   MORTGAGE                 1868\n                   OWN                       354\n                   OTHER                      15\nVENTURE            RENT                     2135\n                   MORTGAGE                 1811\n                   OWN                       648\n                   OTHER                      20\nName: count, dtype: int64\n\n\nWe can see here that there isn’t too much variation, but there are definitely differences that stand out. By far, the most popular type of loan for homeowners is a venture loan, and very few homeowners requested a debt consolidation loan. People with mortgages were spread out pretty evenly across the board, but home improvement had the least mortgagers. By far, home improvement was the least popular type of loan amongst renters. All of these make sense to me for a few reasons. Homeowners gravitated towards venture loans because, in general, they are likely to be more financially stable. Renters did not go for the home improvement loans because they likely don’t have much say in what gets done on the property they are renting. There are so few people in the “other” category that it is hard to pick out trends.\nNext, we are going to prepare the data the same way that we did last time. We will split into x and y, removing the target variables from the training set. We will also get rid of null variables.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(df_train)\nx_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n22907 rows × 26 columns\n\n\n\n\n\nPart C: Build a Model\nNow that the data is properly processed, we can train a logistic regression model. This will follow a similar process to the Penguins blog post, where we loop through all possible combinations of 3 features and add their scores and models to a list.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = ['person_age', 'person_income', 'person_emp_length', \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n    new_score = LR.score(x_train[cols], y_train)\n    scores.append((cols, new_score, LR))\n\nNow, we can find the maximum score in the list of results from the model. We can then split up our results into the attributes, the score, and the logistic regression model.\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['person_emp_length', 'loan_percent_income', 'person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT'], 0.8482559916182826, LogisticRegression())\n\n\nBefore we move on, we should make sure the model doesn’t perform drastically worse on unseen data. Below, we cross-validate our model.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nWe could have gone through and cross-validated all the possible models, but this score is close enough to our initial score that it is okay to continue.In fact, it performed slightly better on the cross-validation than on the training data. Let’s continue.\nBelow, we can extract the w vector (weights) from our best logistic regression model.\n\nw = pd.Series(best_lr.coef_[0])\nw\n\n0   -0.019247\n1    8.281007\n2   -0.735289\n3   -0.107546\n4   -1.795208\n5    0.265245\ndtype: float64\n\n\n\n\nPart D: Find a Threshold\nNow we begin the process of determining what the most profitable threshold is for the bank. We can start by defining our scoring function, which is just going to be the dot product of the weight vector w.\n\ndef score_function(w, x):\n    return x@w\n\nWe can also plot out these scores by frequency:\n\nx_train['scores'] = score_function(w.values, x_train[best_attrs])\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(x_train['scores'], bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nMost of the scores fall between 0 and 2. This gives us an idea for where the threshold may fall as well.\nWe can now define the benefit function. We are going to stick with the function given in the assignment, because I do not know much about bank profits!\n\ndef get_benefit(loan_amt, loan_int_rate, default):\n    loan_int_rate = loan_int_rate/100\n    if default == False:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**10 - loan_amt\n    else:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amt\n    return cost\n\nNext, I added a column into the dataset for c (profit for someone who doesn’t default) and C (profit for someone who defaults). This helps me to be able to filter more easily in the future! I made sure to apply the function to the whole pandas series for each column, since Pandas can handle vector arithmetic.\n\nx_train['c'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], False)\nx_train['C'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], True)\nx_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nscores\nc\nC\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n1.201226\n4613.567568\n-6997.533847\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n2.404884\n2044.334031\n-6426.108799\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0.640802\n493.650464\n-795.445199\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n1.5449\n4028.690420\n-9390.333437\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n2.297004\n2430.522429\n-3211.752128\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow for the actual calculations! Below, we loop through 101 possible threshold values and find the benefit for each one. We maintain an array of thresholds and an array of benefits for easy plotting.\n\nbest_benefit = 0\nbest_threshold = 0\nt_arr = []\nbenefits = []\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 3, 101): \n    y_pred = x_train['scores'] &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0)).mean()\n    fn = ((y_pred == 0) & (y_train == 1)).mean()\n    benefit = x_train['c'][x_train['scores'] &gt;= t].sum()*tn - x_train['C'][x_train['scores'] &gt;= t].sum()*fn\n    t_arr.append(t)\n    benefits.append(benefit)\n    if benefit &gt; best_benefit: \n        best_benefit = benefit\n        best_threshold = t\n\nplt.plot(t_arr, benefits)\nplt.plot(best_threshold, best_benefit, marker=\"o\")\nplt.title(f\"Best benefit per person: ${(best_benefit/len(x_train[x_train[\"scores\"] &lt; best_threshold])).round(2)} \\nBest threshold: {best_threshold}\")\nplt.xlabel(\"Threshold score\")\nplt.ylabel(\"Total benefit in dollars\")\n\nText(0, 0.5, 'Total benefit in dollars')\n\n\n\n\n\n\n\n\n\nWe have outputted this plot which shows us the curve of threshold plotted against benefit! We can also see that our best benefit per borrower is $1,868.62, and our optimal threshold is 1.14. Now, let’s test this model and threshold against our testing data. Hopefully, since we did well with the cross-validation, we will do well on the yet unseen data.\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\nx_test, y_test = prepare_data(df_test)\nx_test['c'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], False)\nx_test['C'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], True)\n\n\nt = best_threshold\n\n# compute the scores\nx_test['scores'] = score_function(w.values, x_test[best_attrs])\npreds = x_test['scores'] &gt;= t\n\n# compute error rates\nFN = ((preds == 0) & (y_test == 1)).mean() \nTN = ((preds == 0) & (y_test == 0)).mean() \n# compute the expected gain\ngain = x_test['c'][x_test['scores'] &gt;= t].sum()*TN  - x_test['c'][x_test['scores'] &gt;= t].sum()*FN\ngain/len(x_test[x_test[\"scores\"] &lt; t])\n\nnp.float64(1361.3320954051185)\n\n\nOur profit per borrower is $1,361.33. Not as much as in our training data, but not bad!\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\n\n1. Is it more difficult for people in certain age groups to access credit under your proposed system?\nLet’s look at a plot and find out.\n\nplot = sns.scatterplot(x_train[x_train[\"person_age\"] &lt; 100], x = \"person_age\", y = \"scores\")\nplot.set_title(\"Scores against age for prospective borrowers\")\n\nText(0.5, 1.0, 'Scores against age for prospective borrowers')\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there is much correlation between score and age.\n\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nLet’s check out the data:\n\ndf_train[\"scores\"] = x_train[\"scores\"]\ndf_train.groupby(\"loan_intent\")[\"scores\"].max()\n\nloan_intent\nDEBTCONSOLIDATION     6.14476\nEDUCATION            6.449154\nHOMEIMPROVEMENT       6.06195\nMEDICAL              5.965717\nPERSONAL             6.137947\nVENTURE               5.89633\nName: scores, dtype: object\n\n\nSo the maximum score for medical is lower than almost all the other maximum scores. However, this may not reflect the average score given to medical borrowers.\n\ndf_train.groupby(\"loan_intent\")[\"scores\"].mean()\n\nloan_intent\nDEBTCONSOLIDATION    1.114447\nEDUCATION             1.00498\nHOMEIMPROVEMENT      0.873225\nMEDICAL              1.100077\nPERSONAL             0.969918\nVENTURE              0.900046\nName: scores, dtype: object\n\n\nHere, we can see that the average score for a medical loan request is actually higher than many of the other categories. In general, the Venture category has a low score for both the max score and the mean score. Debt consolidation tends to score hgih, as does education. Medical flips from the high end (mean) to the low end (max). We can also look at the count of each that got a loan, as opposed to the true default values.\n\ndf_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n/var/folders/sy/9tmrg3gx65vf4qjw8jl8ytsc0000gn/T/ipykernel_82223/2869003493.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  df_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n\nloan_intent\nDEBTCONSOLIDATION    648\nEDUCATION            574\nHOMEIMPROVEMENT      424\nMEDICAL              718\nPERSONAL             534\nVENTURE              468\ndtype: int64\n\n\nHere, we calculated the number of people in each loan intent group that were above the threshold and defaulted. Overall, it seems like the error rate among groups was pretty equal when you consider the size of the groups.\n\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nsns.scatterplot(x = x_train[x_train[\"person_income\"] &lt; 2000000][\"person_income\"], y = x_train[\"scores\"])\n\n\n\n\n\n\n\n\nOverall, it seems like lower-income people are more likely to receive a higher score. This is interesting to me because I would assume those who are higher income would be more likely to pay off their loans. However, maybe interest rates are higher among lower-income people in this dataset. Also, it could have something to do with the reasons people are requesting loans.\n\n\n\nPart G: Write and Reflect\n\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nI think this depends on whether you are thinking from a utilitarian or ethical view of fairness. I also think that fairness is not necessary always ethical in the ways people use it. I believe that it is unfair that people who need help cannot access it, and I think it is sad that profits are often prioritized over people.\nMy definition of fairness: Equity–people who need help get it.\nHowever, I can see how someone would argue that while it is unethical to deny someone a medical loan, it may still be fair. Basing loans off of likelihood of default is an objective way of making the decision, and I think some people see objectivity as fairness.\n\n\n\nDiscussion\nAfter completing this blog post, I had a lot more thoughts about using machine learning to predict human behavior. The first assignment was cool because we could get such high accuracy, but this one (and the in-class examples) made me wonder more about ethics. Predicting human behavior is inherently harder for algorithms, and the consequences of getting it wrong are higher. I generally think that using machine learning in situations like this is tempting, but seems very dangerous. The added probability of being wrong and the added consequences of being wrong generally make for a bad risk management decision. In terms of this case, people who needed money and would have paid on time may get passed over because of an algorithmically generated score."
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post, we will analyze the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station in Antarctica. It contains various types of quantitative and qualitative data about the penguins observed by Gorman et al. To analyze this data, we will train a machine learning model on it, and find the best combination of features to train that model on. We will then test our model against the test dataset and determine its usability. The model we will use is a Logistic Regression model, which uses linear equations to create boundaries and categorizes based on those boundaries.\n\n\nSetup\nFirst we can load in the data as a Pandas dataframe.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nWe can then look at the dataframe we have made:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can shorten the species of the penguins to just the first word, which will make it easier to catalogue them.\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nFigure Creation\nNow that we’ve preprocessed the data a little bit, we can create a few plots to get an idea of what this data means. Below, we use seaborn to create a scatterplot and a bar plot.\n\nfig, ax = plt.subplots(2, 1, figsize = (3.5, 8))\n\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = 'Delta 13 C (o/oo)', hue = \"Species\", style = \"Species\", ax = ax[0])\np2 = sns.barplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = 'Species')\n\n\n\n\n\n\n\n\nFigure 1: A scatterplot showing flipper length against Delta 13 C for three species of penguins. We can see that there is some correlation, but at the edges the species overlap a bit.\nFigure 2: A bar graph showing body mass for each species on each island. Torgerson only contains Adelie, Dream has pretty equal body masses for Adelie and Chinstrap, and Biscoe has Gentoo penguins at a much larger size than the other penguins.\nFrom these figures, we can tell that flipper length differentiates Adelie and Gentoo penguins, and that Delta 13 C differentiates Adelie and Chinstrap penguins. However, we do get some overlap between Chinstrap and Adelie that would reduce model accuracy. The second plot shows that only Adelie penguins were observed on Torgerson, Gentoos were only observed on Biscoe, and Chinstraps were only observed on Dream. Although Adelie penguins are found on all three islands, there is enough separation that we can assume that it would be an informative factor to include.\nNext, let’s group the data into an informative table.\n\ntrain.groupby(\"Species\")[\"Flipper Length (mm)\"].mean()\n\nSpecies\nAdelie       190.084034\nChinstrap    196.000000\nGentoo       216.752577\nName: Flipper Length (mm), dtype: float64\n\n\nHere we can see that Gentoo penguins have much longer flippers on average than the other species! Chinstrap has more than Adelie, but the difference is not as significant. This could indicate that flipper length will be a useful feature for our model.\nNow that we’ve seen some of the data plotted out, we can continue the preprocessing step. The predict() function below drops unnecessary columns, rows/columns with null values, assigns species to integers rather than strings, and then splits the species into a separate data structure, y.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(train)\nx_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nDeciding on attributes\nWe now need to pick the attributes that will yield the best prediction accuracy. To do this, we will use an exhaustive approach, where we check the accuracy of a linear regression model trained on all possible feature combinations. In the code below, we loop through all possible combinations of qualitative and quantitative attributes. In each iteration, we train a model on the different combinations of features, and we compile an array of all the combinations (along with their scores and the model that was trained on them).\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Sex\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n1    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n2    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n3    new_score = LR.score(x_train[cols], y_train)\n4    scores.append((cols, new_score, LR))\n\n\n1\n\nGather the features for this iteration\n\n2\n\nCreate the Logistic Regression model and train it on our selected features\n\n3\n\nScore the trained model\n\n4\n\nAdd the features, score, and model as a tuple to the array of our catalogued models\n\n\n\n\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.99609375, LogisticRegression())\n\n\nIt looks like Island, Culmen Length, and Culmen Depth are our most helpful attributes.\nNext, we need to prepare our test data in the same way that we prepared our training data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\nx_test, y_test = prepare_data(test)\n\nWe can then score our trained model on its performance with the test data.\n\nbest_lr.score(x_test[best_attrs], y_test)\n\n1.0\n\n\nGreat! We have 100% accuracy on the testing data. Now for some more plotting…\n\n\nPlotting the Category Regions\nUsing Matplotlib, we can display the categories on a plot as colored regions. This allows us to visualize how the test points are categorized, and we can get a visual representation of the accuracy of the model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are going to look at both the training data and the testing data, split up by the qualitative factor (the islands).\n\nplot_regions(best_lr, x_train[best_attrs], y_train)\nplot_regions(best_lr, x_test[best_attrs], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the model had no trouble at all with the test data–all the points were clearly within their respective categories on the plot. To continue to test our model against unseen data, we can split up the training data to cross-validate. scikit-learn has a function for this, and it will test our Logistic Regression model against different subsets of the training model.\nCross-validating…\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nOur model did very well on the cross-validation in addition to the training data, with an average score of 99.6%. Another way we can visualize its performance is to create a confusion matrix, which shows us specifically what the model predicted in terms of classification. We will do this with our test data.\nConfusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = best_lr.predict(x_test[best_attrs])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nMore intuitive formatting:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nSince the model was 100% correct on the testing data, it is hard to comment on where it could go wrong. However, I predict that false positives for Adelie could occur, since Adelie penguins are found on all three islands. Additionally, the training data included a few penguins who were Chinstraps that were on the border of the Chinstrap and Adelie classifications in the above regions plot.\n\n\nDiscussion\nIn this blog post, we used a Logistic Regression model to classify penguins from the Palmer Penguins dataset. We found that the most important features were the island they were observed on and their culmen dimensions. In general, the model was successful in categorizing the section of the data that we put aside for testing, and it performed well for being trained on only a few features. I am curious if adding more features would improve the accuracy of the model, or if it would introduce overfitting to the model and would become too specific."
  },
  {
    "objectID": "posts/newton/index.html",
    "href": "posts/newton/index.html",
    "title": "Newtons Method and Adam Optimizers",
    "section": "",
    "text": "Abstract\nIn this blog post, we will implement two advanced optimization techniques and compare them. First, we will implement a Newton’s Method Optimizer, which uses the Hessian matrix of the data as well as the gradient to update the weight vector. This method can converge very quickly, but it is sensitive to saddle points and struggles with certain datasets. Next, we will implement the Adam Optimizer, which operates on a subset of the data, and therefore can complete updates much more quickly. Because of this, however, the Adam Optimizer needs many more steps to converge. Implementations of these algorithms are located on my Github here. This blog post will aim to compare Newton and Adam and determine how much more efficient Adam can be. Do the faster steps fully offset the need for many more iterations?\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport math\nimport time\n\n\n\nPreliminary Testing of Newton’s Method\nFirst, we will take a synthetic dataset and test Newton’s Method. This will hopefully eliminate any errors that could occur because of the data, and give us a good indication that our implementation works.\n\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X.double(), y.double()\n\nX, y = classification_data(noise = 0.5)\n\nWe will create two models: one that uses Newton’s Method as the optimizer and one that uses the Gradient Descent Optimizer. We will then compare and check to see that the w vectors they create are approximately similar.\n\nnewton = NewtonOptimizer()\nloss_arr = []\nloss = torch.inf\nwhile loss &gt; 62.5:\n    loss = newton.loss(X, y)\n    loss_arr.append(loss)\n    newton.step(X, y, alpha = 0.1)\n\n\nGD = GradientDescentOptimizer()\nloss_arr2 = []\nloss = torch.inf\nwhile loss &gt;= 62.5:\n    loss = GD.loss(X, y)\n    loss_arr2.append(loss)\n    GD.step(X, y, alpha = 0.3, beta = 0)\n\n\nloss_arr2[-1]\n\ntensor(62.4998, dtype=torch.float64)\n\n\n\nax = plt.axes()\nax.plot(loss_arr, label = \"Newton\")\nax.plot(loss_arr2, label = \"Gradient Descent\")\nax.legend()\n\n\n\n\n\n\n\n\nGradient Descent took more iterations to reach the same loss! However, they both converged to 62.5. Let’s take a closer look at the Newton loss.\n\nplt.plot(loss_arr2)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nThey both have converged, so let’s plot the w vectors and make sure they are approximately similar.\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, newton.w)\n\n\n\n\n\n\n\n\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, GD.w)\n\n\n\n\n\n\n\n\n\nprint(newton.w, GD.w)\n\ntensor([ 4.0682,  2.9263, -3.0962], dtype=torch.float64) tensor([ 4.0166,  2.9222, -3.0854], dtype=torch.float64)\n\n\nThe w vectors are almost exactly the same! Newton converged with many fewer iterations than Gradient Descent.\n\n\nNewton’s Method Experiments\nIt took me a long time to find a dataset that worked well for Newton’s method. Since the step function finds the inverse of the Hessian Matrix, I was running into issues where the Hessian was singular and therefore could not be inverted. I therefore switched to using torch.pinverse so that the loop would continue to run. This did result in some NaN values in the loss array.\nThe data we’ll be using is data on Titanic passengers and whether or not they survived. This dataset was obtained from Kaggle.\nFirst, I read in the data and did some visualizations to better understand the features included.\n\ndf = pd.read_csv(\"/Users/ellisterling/Documents/spring25/csci0451/ellisterling.github.io/posts/newton/data/Titanic-Dataset.csv\")\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\nHere I chose a subset of the features based on both plotting and running the algorithm to see what affected accuracy.\n\nfilter = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Survived\"]\n\nOne such plot was a count plot of the class that the passengers were in, with separate bars for those who survived and those who didn’t. We can see here that there were many more passengers in third class. First class was most likely to survive, second class was about half and half, and the vast majority of people in third class passed away.\n\nax = sns.countplot(df, x = \"Pclass\", hue = \"Survived\")\nax.set_title(\"Survival by Class\")\n\nText(0.5, 1.0, 'Survival by Class')\n\n\n\n\n\n\n\n\n\nWe can also check the number of people who survived versus didn’t. Below, we can see that less than half the passengers survived.\n\ndf.groupby(\"Survived\")[\"Survived\"].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\nNow we are going to process the data to get it into tensor form so the Newton and Adam Optimizers can hande it.\n\ndf = df[filter]\ndf = df.dropna()\ndf = pd.get_dummies(df)\n\nTrain test split!\n\ndf_train, df_test = train_test_split(df, test_size = 0.3)\nX_train = df_train.drop(\"Survived\", axis=1)\ny_train = df_train[\"Survived\"]\nX_test = df_test.drop(\"Survived\", axis=1)\ny_test = df_test[\"Survived\"]\n\n\nX_train_tensor = torch.from_numpy(X_train.to_numpy().astype(float))\nX_test_tensor = torch.from_numpy(X_test.to_numpy().astype(float))\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n\n\nX_train_tensor = X_train_tensor.double()\ny_train_tensor = y_train_tensor.double()\nX_test_tensor = X_test_tensor.double()\ny_test_tensor = y_test_tensor.double()\n\nNow we can run experiments on this data with the Newton Optimizer.\n\nw_dims = X_train_tensor.size()[1]\n\n\nnewton = NewtonOptimizer()\nloss_arr3 = []\n\n\nnewton_start_time = time.time()\nloss = math.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = newton.loss(X_train_tensor, y_train_tensor)\n    loss_arr3.append(loss)\n    newton.step(X_train_tensor, y_train_tensor, alpha = 0.01)\nnewton_end_time = time.time()\n\n\nplt.plot(torch.arange(1, len(loss_arr3)+1), loss_arr3, color = \"black\")\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Newton Optimizer Loss Convergence\")\n\nText(0.5, 1.0, 'Newton Optimizer Loss Convergence')\n\n\n\n\n\n\n\n\n\nInspect the weight vector:\n\nnewton.w\n\ntensor([-6.7286e-01, -3.6812e-02, -1.1938e-04,  2.9238e+00,  1.1506e+00],\n       dtype=torch.float64)\n\n\n\ny_test_tensor\n\ntensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.],\n       dtype=torch.float64)\n\n\n\npredicted = newton.predict(X_test_tensor)\npredicted\n\ntensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n\n\nLet’s check the accuracy:\n\n#find score\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.7535)\n\n\nPretty good! It would be hard to visualize a 5d vector like this on a plot as we did with the synthetic data, but the score on our data is pretty good. Let’s look at the confusion matrix too:\n\n#find confusion matrix\nfrom sklearn.metrics import confusion_matrix\nC = confusion_matrix(y_test_tensor, predicted)\nC\n\narray([[120,  12],\n       [ 41,  42]])\n\n\nWe have a higher error rate for passengers who survived. Now, let’s make an example where alpha is too large and Newton’s method fails to converge.\n\nnewton = NewtonOptimizer()\nn_loss = []\nfor t in range(1000):\n    loss = newton.loss(X_train_tensor, y_train_tensor)\n    n_loss.append(loss)\n    newton.step(X_train_tensor, y_train_tensor, alpha = 7)\n\n\nplt.plot(n_loss)\n\n\n\n\n\n\n\n\nWe can see from the plot that the model didn’t converge, and does not look to be on the course to converge.\n\npredicted = newton.predict(X_test_tensor)\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.3860)\n\n\nWe can also see that it didn’t reach the level of accuracy it was reaching before.\n\n\nAdam Optimizer Experiments\n\nadam = AdamOptimizer()\nadam_loss = []\n\n\nadam_start_time = time.time()\nt = 1\nloss = torch.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = adam.loss(X_train_tensor, y_train_tensor)\n    adam_loss.append(loss)\n    adam.step(X_train_tensor, y_train_tensor, 10, 0.01, 0.9, 0.999, t)\n    t += 1\nadam_end_time = time.time()\n\n\nplt.plot(adam_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Adam Optimizer Loss Convergence\")\n\nText(0.5, 1.0, 'Adam Optimizer Loss Convergence')\n\n\n\n\n\n\n\n\n\n\npredicted = adam.predict(X_test_tensor)\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.7209)\n\n\nGreat! We are able to obtain similar accuracy by predicting with Adam to Newton. Now, let’s compare against standard minibatch stochastic gradient descent. I did this by selecting a batch in the loop, and using my standard gradient descent model from the Logistic Regression blog post.\n\nsgd = GradientDescentOptimizer()\nsgd_loss = []\nt = 1\nloss = torch.inf\nbatch_size = 10\nfor t in range(20000):\n    i = torch.randint(0, len(y_train_tensor), (batch_size,))\n    loss = sgd.loss(X_train_tensor, y_train_tensor)\n    sgd_loss.append(loss)\n    sgd.step(X_train_tensor[i], y_train_tensor[i], 0.01, 0.9)\n\nLet’s take a look at the loss plot:\n\nplt.plot(sgd_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for SGD with alpha = .01\")\n\nText(0.5, 1.0, 'Convergence for SGD with alpha = .01')\n\n\n\n\n\n\n\n\n\nClearly, at this step size (.01), SGD does not converge. Let’s try with some smaller timesteps.\n\nadam = AdamOptimizer()\nadam_loss = []\n\n\nt = 1\nloss = torch.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = adam.loss(X_train_tensor, y_train_tensor)\n    adam_loss.append(loss)\n    adam.step(X_train_tensor, y_train_tensor, 10, 0.0001, 0.9, 0.999, t)\n    t += 1\n\n\nplt.plot(adam_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for Adam with alpha = 0.0001\")\n\nText(0.5, 1.0, 'Convergence for Adam with alpha = 0.0001')\n\n\n\n\n\n\n\n\n\n\nsgd = GradientDescentOptimizer()\nsgd_loss = []\nt = 1\nloss = torch.inf\nbatch_size = 10\nwhile loss &gt;= 240 or torch.isnan(loss):\n    i = torch.randint(0, len(y_train_tensor), (batch_size,))\n    loss = sgd.loss(X_train_tensor, y_train_tensor)\n    sgd_loss.append(loss)\n    sgd.step(X_train_tensor[i], y_train_tensor[i], 0.0001, 0.9)\n\n\nplt.plot(sgd_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for SGD with alpha = 0.0001\")\n\nText(0.5, 1.0, 'Convergence for SGD with alpha = 0.0001')\n\n\n\n\n\n\n\n\n\nThe plot looks a little weirder than the Adam plot, but it certainly converges. It also completed its training loop in faster than Adam. Let’s plot them together to be able to visualize that:\n\nax = plt.axes()\nax.plot(sgd_loss, label = \"SGD\")\nax.plot(adam_loss, label=\"Adam\")\nax.legend()\n\n\n\n\n\n\n\n\nSGD performs faster with smaller steps, but fails to converge at larger steps. Adam converges more reliably, but takes a longer time with smaller step sizes (e.g. .0001).\n\n\nNewton and Adam Comparison\nIn the sections above, we ran Newton and Adam optimizers until the loss reached 240, which seemed to be about where they converged for this dataset. Adam took more iterations, but frequently beat out Newton in terms of time taken to run to convergence. On occasion, Newton would beat Adam, but this likely depended on the train-test split of the data and the initial choice of the weight vector w for both optimizers. When run with a step size of .01, these were their times:\n\nadam_time = adam_end_time - adam_start_time\nnewton_time = newton_end_time - newton_start_time\nprint(f\"Newton Time: {newton_time}\\nAdam Time: {adam_time}\")\n\nNewton Time: 0.08109903335571289\nAdam Time: 0.03467416763305664\n\n\nGenerally, it seems that Adam is faster, even though it takes more iterations to reach convergence. However, depending on the train-test split and the initial choice of w, Newton may perform better. Both Adam and Newton perform best with small step sizes.\n\n\nDiscussion\nOverall, running these experiments showed me that the optimizer that performs the best is heavily dependent on the dataset and the step size. One thing that interested me about this is how increasing complexity of the optimizer/implementing a more advanced method means that, in some ways, we are sacrificing reliability of convergence. However, these more advanced methods can handle more complex datasets than classic gradient descent. I was also interested that the train-test split and the choice of initial w vectors drastically affected the time and iterations that it took Newton and Adam to converge. Overall, it seemed that Adam was faster, but Newton could overtake it a lot of the time. I would be interested to try these optimizers on a bunch of datasets to get better context for which one has overall better performance."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nAbstract\nIn this blog post, we will implement and test the Perceptron algorithm. This algorithm is one of the oldest machine learning algorithms, and makes up the backbone of many more advanced algorithms that are in use today. Perceptron works by calculating the gradient of a feature matrix and using that gradient to update the weight vector that will eventually separate the data. We will test this on linearly separable data as well as non-linearly separable data, and see how the loss of the function reacts. We will also test our model on 6-dimensional data, or data with 6 features, and see how our model responds.\nThe code implementing Perceptron for this blog post can be found here.\n\n\nPerceptron Intro\nMy Perceptron code is all based around a linear model that contains a weight vector, a score function, and a predict function. The Perceptron functionality comes into play with the Perceptron and PerceptronOptimizer classes. The Perceptron loss() function calculates the percentage of misclassified points by comparing the score of X to the expected result vector y. The Perceptron grad() function approximates the gradient by multiplying X by 2y-1 and then multiplying that by -1 * scores * (2y-1) where scores &lt; 0. Finally, the step function updates w by subtracting the gradient from it.\n\n\nExperiments\nData generation and plotting code adapted from Prof. Phil Chodrow’s lecture notes.\n\nExperiment 1: 2D Linearly Separable Data\nThe first experiment we’ll try is one using data that is 2D and linearly separable (or has two distinct classes that don’t overlap). We want to check that the resulting weight vector divides these classes exactly.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs we can see from this plot, the data is linearly separable. Now, let’s test our Perceptron. It should reach a loss of 0, since there should be a vector that perfectly splits the classes.\n\nopt = PerceptronOptimizer()\nloss = 1.0\nloss_vec = []\nn = X.size()[0]\nwhile loss &gt; 0:\n    loss = opt.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y)\n\n\nplt.plot(loss_vec)\n\n\n\n\n\n\n\n\nWe can see that our loss function converges to 0! Now, let’s get a visualization of our resulting weight vector.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots()\nplot_perceptron_data(X, y, ax)\ndraw_line(opt.w, -0.5, 1.5, ax)\n\n\n\n\n\n\n\n\nPerfect! We can see that the line separates our classes perfectly.\n\n\nExperiment 2: Non-Linearly Separable Data\nNow, we want to test our model’s performance on data that is not linearly separable. Because we will not be able to split this data perfectly, we can expect that our function will not converge to 0.\n\nX, y = perceptron_data(noise=0.5)\nfig, ax = plt.subplots()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs we can see, the data above is not linearly separable. We’ll add another condition to our while loop to make sure that we don’t keep running our model forever–1000 is our max number of iterations.\n\nopt = PerceptronOptimizer()\nloss = 1.0\nloss_vec = []\nn = X.size()[0]\ni = 0\nwhile loss &gt; 0 and i&lt;=1000:\n    loss = opt.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y)\n    i+=1\n\n\nplt.plot(loss_vec)\n\n\n\n\n\n\n\n\nWe can see clearly that it does not converge. However, let’s look at our resulting weight vector.\n\nfig, ax = plt.subplots()\nplot_perceptron_data(X, y, ax)\ndraw_line(opt.w, -0.5, 1.5, ax)\n\n\n\n\n\n\n\n\nIt still looks like it separates our data pretty well! However, if we were to use Perceptron on datasets that weren’t linearly separable (like most real-world data), we would not have a way to tell when the model had reached the best possible vector for w, and we would have to rely on maximum iterations like we do in this experiment.\n\n\nExperiment 3: 6D Data\nNow, let’s try to use our model on data with 6 features.\n\nX, y = perceptron_data(p_dims = 6)\n\n\nopt = PerceptronOptimizer()\nloss = 1.0\nloss_vec = []\nn = X.size()[0]\ni = 0\nwhile loss &gt; 0 and i&lt;=1000:\n    loss = opt.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y)\n    i+=1\n\nAlthough we can’t create a visualization of the vector like we could with the 2D data, we can still take a look at the loss and see if it converges to 0.\n\nplt.plot(loss_vec)\n\n\n\n\n\n\n\n\nWonderful! Our code converges to 0, which indicates that it was able to separate the classes perfectly, and that the data is linearly separable even though it has more features.\n\n\n\nRuntime Complexity\nThe runtime for one step of the Perceptron algorithm would be O(p), where p is the number of features in the data. It does not depend on the number of data points.\n\n\nDiscussion\nIn this blog post, we implemented the Perceptron algorithm. We saw how it worked very well for all linearly separable data, even data with 6 features. In the future, I’d be interested to see how real-world runtime differs if you add many more features (orders of magnitude larger), since the theoretical runtime is certainly affected by features. The model did not converge for data that was not linearly separable. This raises questions about its usefulness for real-world data, since I have almost never encountered real-world datasets that are linearly separable. A lot of the time, there is not one hyperplane that can perfectly separate the classes of data. In these cases, Perceptron could still be used, but the loss won’t converge, so an arbitrary maximum number of iterations must be chosen. This leads us to the Logistic Regression algorithm, which is implemented in another blog post, and allows us to use a model that converges on non-linearly separable data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elli Sterling’s CSCI 0451 Blog",
    "section": "",
    "text": "Attempting to Predict ENSO Phases\n\n\n\n\n\nFinal Project for CSCI0451\n\n\n\n\n\nMay 12, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nNewtons Method and Adam Optimizers\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing bias with machine learning models.\n\n\n\n\n\nMar 8, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nDesigning an algorithm to decide whether or not loan applicants should receive loans.\n\n\n\n\n\nFeb 27, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying penguins by species based on the Palmer Penguins dataset\n\n\n\n\n\nFeb 12, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\nNo matching items"
  }
]