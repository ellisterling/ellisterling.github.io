[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer"
  },
  {
    "objectID": "posts/auditing_bias/index.html",
    "href": "posts/auditing_bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n60\n15.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n1\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n18.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n20\n18.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n33\n18.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(99419, 15)\n(99419,)\n(99419,)\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\nnp.float64(0.7863608931804466)\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\nnp.float64(0.7875706214689265)\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n\nnp.float64(0.7777164920022063)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n23.0\n21.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n1\n24.0\n18.0\n5.0\n2.0\n2.0\n0.0\n1.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2\nTrue\n\n\n2\n33.0\n13.0\n3.0\n13.0\n1.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n1.0\n1.0\n1\nTrue\n\n\n3\n88.0\n19.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n4\n4.0\n2.0\n5.0\n2.0\n2.0\n5.0\n1.0\n3.0\n0.0\n4.0\n1.0\n2.0\n2.0\n0.0\n2.0\n1\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n79530\n50.0\n21.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n1.0\n2\nTrue\n\n\n79531\n70.0\n16.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79532\n18.0\n16.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79533\n78.0\n16.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n79534\n22.0\n16.0\n5.0\n2.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n\n\n79535 rows × 17 columns\n\n\n\nHow many individuals are in the dataset?\n\nlen(df)\n\n79535\n\n\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1? ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍In ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employment ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍prediction, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍would ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍correspond ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals.\n\nprint(df[\"label\"].value_counts())\nprint(35232/(35232+44303))\n\nlabel\nFalse    44303\nTrue     35232\nName: count, dtype: int64\n0.44297479097252784\n\n\n44.3%\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍how ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍many ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍groups?\n\ndf[\"group\"].value_counts()\n\ngroup\n1    67415\n2     6881\n6     2061\n9     1922\n8      670\n3      467\n5       95\n7       24\nName: count, dtype: int64\n\n\nIn ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1?\n\n(df[df[\"label\"] == True][\"group\"].value_counts())/df[\"group\"].value_counts()\n\ngroup\n1    0.454825\n2    0.345880\n6    0.492479\n9    0.328824\n8    0.444776\n3    0.419700\n5    0.400000\n7    0.458333\nName: count, dtype: float64\n\n\nCheck ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍intersectional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍trends ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍studying ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍broken ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍out ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chosen ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍an ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍additional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍For ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍example, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍if ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(RAC1P) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍then ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍could ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍choose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(SEX) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍compute ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍both ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍might ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍good ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍opportunity ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍use ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍visualization ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍such ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bar ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chart, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍e.g. via ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍seaborn package.\n\n(df[df[\"label\"] == True].groupby(\"group\")[\"SEX\"].value_counts())/df.groupby(\"group\")[\"SEX\"].value_counts()\n\ngroup  SEX\n1      1.0    0.486367\n       2.0    0.423627\n2      1.0    0.308653\n       2.0    0.379828\n3      1.0    0.434389\n       2.0    0.406504\n5      1.0    0.360000\n       2.0    0.444444\n6      1.0    0.573034\n       2.0    0.419593\n7      1.0    0.400000\n       2.0    0.500000\n8      1.0    0.495575\n       2.0    0.392749\n9      1.0    0.323656\n       2.0    0.333669\nName: count, dtype: float64\n\n\nNext step is to make a plot where the x axis is race, y is percentage positive labels, and then the hue of the bar would be indicative of sex. From the pure numbers, we can see that group 2, which represents African Americans in the dataset, is much lower than many of the other groups.\n\nimport seaborn as sns\nsns.barplot(x = \"group\", y = , data = df, hue = \"SEX\")\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[41], line 2\n      1 import seaborn as sns\n----&gt; 2 sns.barplot(x = df[\"group\"], y = percentages, hue = df[\"SEX\"])\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.13/site-packages/seaborn/categorical.py:2755, in barplot(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge, ci, ax, **kwargs)\n   2752 if estimator is len:\n   2753     estimator = \"size\"\n-&gt; 2755 plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n   2756                       estimator, errorbar, n_boot, units, seed,\n   2757                       orient, color, palette, saturation,\n   2758                       width, errcolor, errwidth, capsize, dodge)\n   2760 if ax is None:\n   2761     ax = plt.gca()\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.13/site-packages/seaborn/categorical.py:1533, in _BarPlotter.__init__(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\n   1530 self.establish_variables(x, y, hue, data, orient,\n   1531                          order, hue_order, units)\n   1532 self.establish_colors(color, palette, saturation)\n-&gt; 1533 self.estimate_statistic(estimator, errorbar, n_boot, seed)\n   1535 self.dodge = dodge\n   1536 self.width = width\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.13/site-packages/seaborn/categorical.py:1480, in _CategoricalStatPlotter.estimate_statistic(self, estimator, errorbar, n_boot, seed)\n   1477     continue\n   1479 hue_mask = self.plot_hues[i] == hue_level\n-&gt; 1480 df = pd.DataFrame({var: group_data[hue_mask]})\n   1481 if self.plot_units is not None:\n   1482     df[\"units\"] = self.plot_units[i][hue_mask]\n\nIndexError: boolean index did not match indexed array along axis 0; size of axis is 0 but size of corresponding boolean axis is 67415"
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post, we will analyze the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station in Antarctica. It contains various types of quantitative and qualitative data about the penguins observed by Gorman et al. To analyze this data, we will train a machine learning model on it, and find the best combination of features to train that model on. We will then test our model against the test dataset and determine its usability. The model we will use is a Logistic Regression model, which uses linear equations to create boundaries and categorizes based on those boundaries.\n\n\nSetup\nFirst we can load in the data as a Pandas dataframe.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nWe can then look at the dataframe we have made:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can shorten the species of the penguins to just the first word, which will make it easier to catalogue them.\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nFigure Creation\nNow that we’ve preprocessed the data a little bit, we can create a few plots to get an idea of what this data means. Below, we use seaborn to create a scatterplot and a bar plot.\n\nfig, ax = plt.subplots(2, 1, figsize = (3.5, 8))\n\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = 'Delta 13 C (o/oo)', hue = \"Species\", style = \"Species\", ax = ax[0])\np2 = sns.barplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = 'Species')\n\n\n\n\n\n\n\n\nFigure 1: A scatterplot showing flipper length against Delta 13 C for three species of penguins. We can see that there is some correlation, but at the edges the species overlap a bit.\nFigure 2: A bar graph showing body mass for each species on each island. Torgerson only contains Adelie, Dream has pretty equal body masses for Adelie and Chinstrap, and Biscoe has Gentoo penguins at a much larger size than the other penguins.\nFrom these figures, we can tell that flipper length differentiates Adelie and Gentoo penguins, and that Delta 13 C differentiates Adelie and Chinstrap penguins. However, we do get some overlap between Chinstrap and Adelie that would reduce model accuracy. The second plot shows that only Adelie penguins were observed on Torgerson, Gentoos were only observed on Biscoe, and Chinstraps were only observed on Dream. Although Adelie penguins are found on all three islands, there is enough separation that we can assume that it would be an informative factor to include.\nNext, let’s group the data into an informative table.\n\ntrain.groupby(\"Species\")[\"Flipper Length (mm)\"].mean()\n\nSpecies\nAdelie       190.084034\nChinstrap    196.000000\nGentoo       216.752577\nName: Flipper Length (mm), dtype: float64\n\n\nHere we can see that Gentoo penguins have much longer flippers on average than the other species! Chinstrap has more than Adelie, but the difference is not as significant. This could indicate that flipper length will be a useful feature for our model.\nNow that we’ve seen some of the data plotted out, we can continue the preprocessing step. The predict() function below drops unnecessary columns, rows/columns with null values, assigns species to integers rather than strings, and then splits the species into a separate data structure, y.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(train)\nx_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nDeciding on attributes\nWe now need to pick the attributes that will yield the best prediction accuracy. To do this, we will use an exhaustive approach, where we check the accuracy of a linear regression model trained on all possible feature combinations. In the code below, we loop through all possible combinations of qualitative and quantitative attributes. In each iteration, we train a model on the different combinations of features, and we compile an array of all the combinations (along with their scores and the model that was trained on them).\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Sex\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n1    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n2    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n3    new_score = LR.score(x_train[cols], y_train)\n4    scores.append((cols, new_score, LR))\n\n\n1\n\nGather the features for this iteration\n\n2\n\nCreate the Logistic Regression model and train it on our selected features\n\n3\n\nScore the trained model\n\n4\n\nAdd the features, score, and model as a tuple to the array of our catalogued models\n\n\n\n\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.99609375, LogisticRegression())\n\n\nIt looks like Island, Culmen Length, and Culmen Depth are our most helpful attributes.\nNext, we need to prepare our test data in the same way that we prepared our training data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\nx_test, y_test = prepare_data(test)\n\nWe can then score our trained model on its performance with the test data.\n\nbest_lr.score(x_test[best_attrs], y_test)\n\n1.0\n\n\nGreat! We have 100% accuracy on the testing data. Now for some more plotting…\n\n\nPlotting the Category Regions\nUsing Matplotlib, we can display the categories on a plot as colored regions. This allows us to visualize how the test points are categorized, and we can get a visual representation of the accuracy of the model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are going to look at both the training data and the testing data, split up by the qualitative factor (the islands).\n\nplot_regions(best_lr, x_train[best_attrs], y_train)\nplot_regions(best_lr, x_test[best_attrs], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the model had no trouble at all with the test data–all the points were clearly within their respective categories on the plot. To continue to test our model against unseen data, we can split up the training data to cross-validate. scikit-learn has a function for this, and it will test our Logistic Regression model against different subsets of the training model.\nCross-validating…\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nOur model did very well on the cross-validation in addition to the training data, with an average score of 99.6%. Another way we can visualize its performance is to create a confusion matrix, which shows us specifically what the model predicted in terms of classification. We will do this with our test data.\nConfusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = best_lr.predict(x_test[best_attrs])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nMore intuitive formatting:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nSince the model was 100% correct on the testing data, it is hard to comment on where it could go wrong. However, I predict that false positives for Adelie could occur, since Adelie penguins are found on all three islands. Additionally, the training data included a few penguins who were Chinstraps that were on the border of the Chinstrap and Adelie classifications in the above regions plot.\n\n\nDiscussion\nIn this blog post, we used a Logistic Regression model to classify penguins from the Palmer Penguins dataset. We found that the most important features were the island they were observed on and their culmen dimensions. In general, the model was successful in categorizing the section of the data that we put aside for testing, and it performed well for being trained on only a few features. I am curious if adding more features would improve the accuracy of the model, or if it would introduce overfitting to the model and would become too specific."
  },
  {
    "objectID": "posts/ADS/index.html",
    "href": "posts/ADS/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this blog post, we will walk through the process of training a linear regression model, creating a scoring function, and finding an optimal threshold for a scoring and classification problem. The dataset we are working with is one with data about prospective loan borrowers from a bank. It includes various characteristics of each person and the loan they are requesting. We will use this data to determine a threshold and scoring function to determine whether or not, if given the loan, the borrower is likely to default (or, in laymans terms, violate the terms of the loan by not paying/causing the bank to lose money). We will use the scikit-learn library again for this blog post. First, we will train a logistic regression model. Then we will find optimal values for the weight vector and the threshold so that we can determine the optimal profit/borrower.\n\n\nPart A: Grab the Data\nTo begin, let’s import the data:\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nWe can take a look at the columns so that we know what attributes we are working with.\n\ndf_train.columns\n\nIndex(['person_age', 'person_income', 'person_home_ownership',\n       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n       'loan_int_rate', 'loan_status', 'loan_percent_income',\n       'cb_person_default_on_file', 'cb_person_cred_hist_length'],\n      dtype='object')\n\n\n\n\nPart B: Explore The Data\nNow, we can make a few figures to explore what the data is saying. First, I created a scatterplot to show the relationship between a person’s age and their credit history length. As can be inferred, older people generally have longer credit histories.\n\nfig, ax = plt.subplots(1, 1)\n# cut out the huge outlier--person making 6m/yr--so that we can see patterns more clearly\np1 = sns.scatterplot(df_train[df_train[\"person_age\"] &lt; 100], x = \"cb_person_cred_hist_length\", y = 'person_age', hue = 'cb_person_default_on_file', style = 'cb_person_default_on_file')\n\n\n\n\n\n\n\n\nHowever, I was very interested in the fact that whether or not people defaulted on their loan seemed not to be reliant on age. There are orange x’s in all parts of the plot. Because of this, I wanted to look at the mean age for people who had a history of defaulting and people who didn’t.\n\ndf_train.groupby(\"cb_person_default_on_file\")[\"person_age\"].mean()\n\ncb_person_default_on_file\nN    27.721878\nY    27.793096\nName: person_age, dtype: float64\n\n\nAs you can see, although the average age for people who had defaulted was slightly bigger, they still both had averages of about 27. This was surprising to me because I had previously assumed that people who are older, and therefore have longer credit histories, would be more likely to have defaulted on a loan. I wonder if this is due to financial differences between generations.\nNext, I wanted to look at interest rate based on history of defaulting. I assumed before making the plot that people who had defaulted before would be given a higher interest rate, and I was correct.\n\nbarplot = sns.barplot(df_train, x = \"cb_person_default_on_file\", y = \"loan_int_rate\")\nbarplot.set_title(\"Average loan interest rate separated by previous loan defaults\")\n\nText(0.5, 1.0, 'Average loan interest rate separated by previous loan defaults')\n\n\n\n\n\n\n\n\n\nHere, we can see that the average loan interest rate for those who had not defaulted on a loan previously is about 10%. For those who had, it was about 15%. A 5% increase is quite significant, and it seems like having a clean loan history would significantly benefit someone’s chances at getting a good interest rate.\nNext, I wanted to look at the factors which played into loan intent. To do this, I created a table showing the home ownership counts per loan intent.\n\ndf_train.groupby(\"loan_intent\")[\"person_home_ownership\"].value_counts()\n\nloan_intent        person_home_ownership\nDEBTCONSOLIDATION  RENT                     2260\n                   MORTGAGE                 1841\n                   OWN                        62\n                   OTHER                      15\nEDUCATION          RENT                     2612\n                   MORTGAGE                 2089\n                   OWN                       412\n                   OTHER                      14\nHOMEIMPROVEMENT    MORTGAGE                 1384\n                   RENT                     1252\n                   OWN                       255\n                   OTHER                      11\nMEDICAL            RENT                     2740\n                   MORTGAGE                 1730\n                   OWN                       352\n                   OTHER                      13\nPERSONAL           RENT                     2171\n                   MORTGAGE                 1868\n                   OWN                       354\n                   OTHER                      15\nVENTURE            RENT                     2135\n                   MORTGAGE                 1811\n                   OWN                       648\n                   OTHER                      20\nName: count, dtype: int64\n\n\nWe can see here that there isn’t too much variation, but there are definitely differences that stand out. By far, the most popular type of loan for homeowners is a venture loan, and very few homeowners requested a debt consolidation loan. People with mortgages were spread out pretty evenly across the board, but home improvement had the least mortgagers. By far, home improvement was the least popular type of loan amongst renters. All of these make sense to me for a few reasons. Homeowners gravitated towards venture loans because, in general, they are likely to be more financially stable. Renters did not go for the home improvement loans because they likely don’t have much say in what gets done on the property they are renting. There are so few people in the “other” category that it is hard to pick out trends.\nNext, we are going to prepare the data the same way that we did last time. We will split into x and y, removing the target variables from the training set. We will also get rid of null variables.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(df_train)\nx_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n22907 rows × 26 columns\n\n\n\n\n\nPart C: Build a Model\nNow that the data is properly processed, we can train a logistic regression model. This will follow a similar process to the Penguins blog post, where we loop through all possible combinations of 3 features and add their scores and models to a list.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = ['person_age', 'person_income', 'person_emp_length', \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n    new_score = LR.score(x_train[cols], y_train)\n    scores.append((cols, new_score, LR))\n\nNow, we can find the maximum score in the list of results from the model. We can then split up our results into the attributes, the score, and the logistic regression model.\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['person_emp_length', 'loan_percent_income', 'person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT'], 0.8482559916182826, LogisticRegression())\n\n\nBefore we move on, we should make sure the model doesn’t perform drastically worse on unseen data. Below, we cross-validate our model.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nWe could have gone through and cross-validated all the possible models, but this score is close enough to our initial score that it is okay to continue.In fact, it performed slightly better on the cross-validation than on the training data. Let’s continue.\nBelow, we can extract the w vector (weights) from our best logistic regression model.\n\nw = pd.Series(best_lr.coef_[0])\nw\n\n0   -0.019247\n1    8.281007\n2   -0.735289\n3   -0.107546\n4   -1.795208\n5    0.265245\ndtype: float64\n\n\n\n\nPart D: Find a Threshold\nNow we begin the process of determining what the most profitable threshold is for the bank. We can start by defining our scoring function, which is just going to be the dot product of the weight vector w.\n\ndef score_function(w, x):\n    return x@w\n\nWe can also plot out these scores by frequency:\n\nx_train['scores'] = score_function(w.values, x_train[best_attrs])\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(x_train['scores'], bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nMost of the scores fall between 0 and 2. This gives us an idea for where the threshold may fall as well.\nWe can now define the benefit function. We are going to stick with the function given in the assignment, because I do not know much about bank profits!\n\ndef get_benefit(loan_amt, loan_int_rate, default):\n    loan_int_rate = loan_int_rate/100\n    if default == False:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**10 - loan_amt\n    else:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amt\n    return cost\n\nNext, I added a column into the dataset for c (profit for someone who doesn’t default) and C (profit for someone who defaults). This helps me to be able to filter more easily in the future! I made sure to apply the function to the whole pandas series for each column, since Pandas can handle vector arithmetic.\n\nx_train['c'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], False)\nx_train['C'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], True)\nx_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nscores\nc\nC\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n1.201226\n4613.567568\n-6997.533847\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n2.404884\n2044.334031\n-6426.108799\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0.640802\n493.650464\n-795.445199\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n1.5449\n4028.690420\n-9390.333437\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n2.297004\n2430.522429\n-3211.752128\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow for the actual calculations! Below, we loop through 101 possible threshold values and find the benefit for each one. We maintain an array of thresholds and an array of benefits for easy plotting.\n\nbest_benefit = 0\nbest_threshold = 0\nt_arr = []\nbenefits = []\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 3, 101): \n    y_pred = x_train['scores'] &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0)).mean()\n    fn = ((y_pred == 0) & (y_train == 1)).mean()\n    benefit = x_train['c'][x_train['scores'] &gt;= t].sum()*tn - x_train['C'][x_train['scores'] &gt;= t].sum()*fn\n    t_arr.append(t)\n    benefits.append(benefit)\n    if benefit &gt; best_benefit: \n        best_benefit = benefit\n        best_threshold = t\n\nplt.plot(t_arr, benefits)\nplt.plot(best_threshold, best_benefit, marker=\"o\")\nplt.title(f\"Best benefit per person: ${(best_benefit/len(x_train[x_train[\"scores\"] &lt; best_threshold])).round(2)} \\nBest threshold: {best_threshold}\")\nplt.xlabel(\"Threshold score\")\nplt.ylabel(\"Total benefit in dollars\")\n\nText(0, 0.5, 'Total benefit in dollars')\n\n\n\n\n\n\n\n\n\nWe have outputted this plot which shows us the curve of threshold plotted against benefit! We can also see that our best benefit per borrower is $1,868.62, and our optimal threshold is 1.14. Now, let’s test this model and threshold against our testing data. Hopefully, since we did well with the cross-validation, we will do well on the yet unseen data.\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\nx_test, y_test = prepare_data(df_test)\nx_test['c'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], False)\nx_test['C'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], True)\n\n\nt = best_threshold\n\n# compute the scores\nx_test['scores'] = score_function(w.values, x_test[best_attrs])\npreds = x_test['scores'] &gt;= t\n\n# compute error rates\nFN = ((preds == 0) & (y_test == 1)).mean() \nTN = ((preds == 0) & (y_test == 0)).mean() \n# compute the expected gain\ngain = x_test['c'][x_test['scores'] &gt;= t].sum()*TN  - x_test['c'][x_test['scores'] &gt;= t].sum()*FN\ngain/len(x_test[x_test[\"scores\"] &lt; t])\n\nnp.float64(1361.3320954051185)\n\n\nOur profit per borrower is $1,361.33. Not as much as in our training data, but not bad!\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\n\n1. Is it more difficult for people in certain age groups to access credit under your proposed system?\nLet’s look at a plot and find out.\n\nplot = sns.scatterplot(x_train[x_train[\"person_age\"] &lt; 100], x = \"person_age\", y = \"scores\")\nplot.set_title(\"Scores against age for prospective borrowers\")\n\nText(0.5, 1.0, 'Scores against age for prospective borrowers')\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there is much correlation between score and age.\n\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nLet’s check out the data:\n\ndf_train[\"scores\"] = x_train[\"scores\"]\ndf_train.groupby(\"loan_intent\")[\"scores\"].max()\n\nloan_intent\nDEBTCONSOLIDATION     6.14476\nEDUCATION            6.449154\nHOMEIMPROVEMENT       6.06195\nMEDICAL              5.965717\nPERSONAL             6.137947\nVENTURE               5.89633\nName: scores, dtype: object\n\n\nSo the maximum score for medical is lower than almost all the other maximum scores. However, this may not reflect the average score given to medical borrowers.\n\ndf_train.groupby(\"loan_intent\")[\"scores\"].mean()\n\nloan_intent\nDEBTCONSOLIDATION    1.114447\nEDUCATION             1.00498\nHOMEIMPROVEMENT      0.873225\nMEDICAL              1.100077\nPERSONAL             0.969918\nVENTURE              0.900046\nName: scores, dtype: object\n\n\nHere, we can see that the average score for a medical loan request is actually higher than many of the other categories. In general, the Venture category has a low score for both the max score and the mean score. Debt consolidation tends to score hgih, as does education. Medical flips from the high end (mean) to the low end (max). We can also look at the count of each that got a loan, as opposed to the true default values.\n\ndf_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n/var/folders/sy/9tmrg3gx65vf4qjw8jl8ytsc0000gn/T/ipykernel_82223/2869003493.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  df_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n\nloan_intent\nDEBTCONSOLIDATION    648\nEDUCATION            574\nHOMEIMPROVEMENT      424\nMEDICAL              718\nPERSONAL             534\nVENTURE              468\ndtype: int64\n\n\nHere, we calculated the number of people in each loan intent group that were above the threshold and defaulted. Overall, it seems like the error rate among groups was pretty equal when you consider the size of the groups.\n\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nsns.scatterplot(x = x_train[x_train[\"person_income\"] &lt; 2000000][\"person_income\"], y = x_train[\"scores\"])\n\n\n\n\n\n\n\n\nOverall, it seems like lower-income people are more likely to receive a higher score. This is interesting to me because I would assume those who are higher income would be more likely to pay off their loans. However, maybe interest rates are higher among lower-income people in this dataset. Also, it could have something to do with the reasons people are requesting loans.\n\n\n\nPart G: Write and Reflect\n\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nI think this depends on whether you are thinking from a utilitarian or ethical view of fairness. I also think that fairness is not necessary always ethical in the ways people use it. I believe that it is unfair that people who need help cannot access it, and I think it is sad that profits are often prioritized over people.\nMy definition of fairness: Equity–people who need help get it.\nHowever, I can see how someone would argue that while it is unethical to deny someone a medical loan, it may still be fair. Basing loans off of likelihood of default is an objective way of making the decision, and I think some people see objectivity as fairness.\n\n\n\nDiscussion\nAfter completing this blog post, I had a lot more thoughts about using machine learning to predict human behavior. The first assignment was cool because we could get such high accuracy, but this one (and the in-class examples) made me wonder more about ethics. Predicting human behavior is inherently harder for algorithms, and the consequences of getting it wrong are higher. I generally think that using machine learning in situations like this is tempting, but seems very dangerous. The added probability of being wrong and the added consequences of being wrong generally make for a bad risk management decision. In terms of this case, people who needed money and would have paid on time may get passed over because of an algorithmically generated score."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer()\nloss_arr = []\nfor _ in range(100):\n    # add other stuff to e.g. keep track of the loss over time.\n    loss = opt.loss(X, y)\n    loss_arr.append(loss)\n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\nplt.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\nImplement Vanilla Gradient Descent\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, opt.w)"
  },
  {
    "objectID": "posts/newton/index.html",
    "href": "posts/newton/index.html",
    "title": "Newtons Method and Adam",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n\nPreliminary Testing of Newton’s Method\n\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X.double(), y.double()\n\nX, y = classification_data(noise = 0.5)\n\n\nopt = NewtonOptimizer()\nloss_arr = []\nfor _ in range(3300):\n    loss = opt.loss(X, y)\n    loss_arr.append(loss)\n    opt.step(X, y, alpha = 0.1)\n\n\nopt2 = GradientDescentOptimizer()\nloss_arr2 = []\nfor _ in range(120):\n    loss = opt2.loss(X, y)\n    loss_arr2.append(loss)\n    opt2.step(X, y, alpha = 0.1, beta = 0)\n\n\nplt.plot(loss_arr)\n\n\n\n\n\n\n\n\n\nplt.plot(loss_arr2)\n\n\n\n\n\n\n\n\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, opt.w)\n\n\n\n\n\n\n\n\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, opt2.w)\n\n\n\n\n\n\n\n\n\nprint(opt.w, opt2.w)\n\ntensor([ 1.6637,  1.3060, -0.7354], dtype=torch.float64) tensor([ 1.4493,  1.3094, -0.9240], dtype=torch.float64)\n\n\nVery similar w values.\n\n\nNewton’s Method Experiments\nIt took me a long time to find a dataset that worked well for Newton’s method. Since the step function finds the inverse of the Hessian Matrix, I was running into issues where the Hessian was singular and therefore could not be inverted. I tried using pinverse to approximate the inverse, but unfortunately that slowed down the algorithm drastically.\n\ndf = pd.read_csv(\"/Users/ellisterling/Documents/spring25/csci0451/ellisterling.github.io/posts/newton/data/Titanic-Dataset.csv\")\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\nfilter = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Survived\"]\n\n\n# fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n# hist = ax.hist(df['Location'], bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\n# |sns.countplot(df[df[\"Preference\"] == 0], x = \"Environmental_Concerns\", hue = \"Preference\")\n# sns.scatterplot(df, x = \"Proximity_to_Mountains\", y = \"Vacation_Budget\", hue = \"Preference\")\nsns.countplot(df, x = \"Pclass\", hue = \"Survived\")\n\n\n\n\n\n\n\n\n\ndf.groupby(\"Survived\")[\"Survived\"].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\n\ndf = df[filter]\ndf = df.dropna()\ndf = pd.get_dummies(df)\ndf[\"Survived\"]\n\n0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n885    0\n886    0\n887    1\n889    1\n890    0\nName: Survived, Length: 714, dtype: int64\n\n\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\nX_train = df_train.drop(\"Survived\", axis=1)\ny_train = df_train[\"Survived\"]\nX_test = df_test.drop(\"Survived\", axis=1)\ny_test = df_test[\"Survived\"]\n\n\nX_train_tensor = torch.from_numpy(X_train.to_numpy().astype(float))\nX_test_tensor = torch.from_numpy(X_test.to_numpy().astype(float))\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n\n\nX_train_tensor = X_train_tensor.double()\nX_train_tensor\ny_train_tensor = y_train_tensor.double()\nX_test_tensor = X_test_tensor.double()\ny_test_tensor = y_test_tensor.double()\n\n\nopt3 = NewtonOptimizer()\nloss_arr = []\nX_train\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nSex_female\nSex_male\n\n\n\n\n532\n3\n17.0\n1\n1\n7.2292\nFalse\nTrue\n\n\n338\n3\n45.0\n0\n0\n8.0500\nFalse\nTrue\n\n\n840\n3\n20.0\n0\n0\n7.9250\nFalse\nTrue\n\n\n151\n1\n22.0\n1\n0\n66.6000\nTrue\nFalse\n\n\n705\n2\n39.0\n0\n0\n26.0000\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n3\n27.0\n0\n0\n7.7958\nFalse\nTrue\n\n\n155\n1\n51.0\n0\n1\n61.3792\nFalse\nTrue\n\n\n285\n3\n33.0\n0\n0\n8.6625\nFalse\nTrue\n\n\n881\n3\n33.0\n0\n0\n7.8958\nFalse\nTrue\n\n\n108\n3\n38.0\n0\n0\n7.8958\nFalse\nTrue\n\n\n\n\n571 rows × 7 columns\n\n\n\n\nopt3.w\n\n\nfor _ in range(5000):\n    # add other stuff to e.g. keep track of the loss over time.\n    loss = opt3.loss(X_train_tensor, y_train_tensor)\n    loss_arr.append(loss)\n    opt3.step(X_train_tensor, y_train_tensor, alpha = 0.25)\n\n\nplt.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"black\")\n\n\n\n\n\n\n\n\n\nplt.plot(loss_arr[100:])\n\n\n\n\n\n\n\n\n\nloss_arr[2000:]\n\n[tensor(310.5317, dtype=torch.float64),\n tensor(310.4860, dtype=torch.float64),\n tensor(310.4403, dtype=torch.float64),\n tensor(310.3947, dtype=torch.float64),\n tensor(310.3490, dtype=torch.float64),\n tensor(310.3035, dtype=torch.float64),\n tensor(310.2579, dtype=torch.float64),\n tensor(310.2125, dtype=torch.float64),\n tensor(310.1670, dtype=torch.float64),\n tensor(310.1216, dtype=torch.float64),\n tensor(310.0762, dtype=torch.float64),\n tensor(310.0309, dtype=torch.float64),\n tensor(309.9856, dtype=torch.float64),\n tensor(309.9404, dtype=torch.float64),\n tensor(309.8952, dtype=torch.float64),\n tensor(309.8501, dtype=torch.float64),\n tensor(309.8050, dtype=torch.float64),\n tensor(309.7599, dtype=torch.float64),\n tensor(309.7149, dtype=torch.float64),\n tensor(309.6699, dtype=torch.float64),\n tensor(309.6249, dtype=torch.float64),\n tensor(309.5800, dtype=torch.float64),\n tensor(309.5352, dtype=torch.float64),\n tensor(309.4904, dtype=torch.float64),\n tensor(309.4456, dtype=torch.float64),\n tensor(309.4008, dtype=torch.float64),\n tensor(309.3562, dtype=torch.float64),\n tensor(309.3115, dtype=torch.float64),\n tensor(309.2669, dtype=torch.float64),\n tensor(309.2223, dtype=torch.float64),\n tensor(309.1778, dtype=torch.float64),\n tensor(309.1333, dtype=torch.float64),\n tensor(309.0888, dtype=torch.float64),\n tensor(309.0444, dtype=torch.float64),\n tensor(309.0001, dtype=torch.float64),\n tensor(308.9557, dtype=torch.float64),\n tensor(308.9115, dtype=torch.float64),\n tensor(308.8672, dtype=torch.float64),\n tensor(308.8230, dtype=torch.float64),\n tensor(308.7788, dtype=torch.float64),\n tensor(308.7347, dtype=torch.float64),\n tensor(308.6906, dtype=torch.float64),\n tensor(308.6466, dtype=torch.float64),\n tensor(308.6026, dtype=torch.float64),\n tensor(308.5586, dtype=torch.float64),\n tensor(308.5147, dtype=torch.float64),\n tensor(308.4708, dtype=torch.float64),\n tensor(308.4270, dtype=torch.float64),\n tensor(308.3832, dtype=torch.float64),\n tensor(308.3394, dtype=torch.float64),\n tensor(308.2957, dtype=torch.float64),\n tensor(308.2520, dtype=torch.float64),\n tensor(308.2084, dtype=torch.float64),\n tensor(308.1648, dtype=torch.float64),\n tensor(308.1212, dtype=torch.float64),\n tensor(308.0777, dtype=torch.float64),\n tensor(308.0342, dtype=torch.float64),\n tensor(307.9908, dtype=torch.float64),\n tensor(307.9474, dtype=torch.float64),\n tensor(307.9040, dtype=torch.float64),\n tensor(307.8607, dtype=torch.float64),\n tensor(307.8174, dtype=torch.float64),\n tensor(307.7742, dtype=torch.float64),\n tensor(307.7310, dtype=torch.float64),\n tensor(307.6878, dtype=torch.float64),\n tensor(307.6447, dtype=torch.float64),\n tensor(307.6016, dtype=torch.float64),\n tensor(307.5586, dtype=torch.float64),\n tensor(307.5156, dtype=torch.float64),\n tensor(307.4726, dtype=torch.float64),\n tensor(307.4297, dtype=torch.float64),\n tensor(307.3868, dtype=torch.float64),\n tensor(307.3439, dtype=torch.float64),\n tensor(307.3011, dtype=torch.float64),\n tensor(307.2583, dtype=torch.float64),\n tensor(307.2156, dtype=torch.float64),\n tensor(307.1729, dtype=torch.float64),\n tensor(307.1303, dtype=torch.float64),\n tensor(307.0876, dtype=torch.float64),\n tensor(307.0451, dtype=torch.float64),\n tensor(307.0025, dtype=torch.float64),\n tensor(306.9600, dtype=torch.float64),\n tensor(306.9176, dtype=torch.float64),\n tensor(306.8751, dtype=torch.float64),\n tensor(306.8327, dtype=torch.float64),\n tensor(306.7904, dtype=torch.float64),\n tensor(306.7481, dtype=torch.float64),\n tensor(306.7058, dtype=torch.float64),\n tensor(306.6636, dtype=torch.float64),\n tensor(306.6214, dtype=torch.float64),\n tensor(306.5792, dtype=torch.float64),\n tensor(306.5371, dtype=torch.float64),\n tensor(306.4950, dtype=torch.float64),\n tensor(306.4530, dtype=torch.float64),\n tensor(306.4110, dtype=torch.float64),\n tensor(306.3690, dtype=torch.float64),\n tensor(306.3271, dtype=torch.float64),\n tensor(306.2852, dtype=torch.float64),\n tensor(306.2434, dtype=torch.float64),\n tensor(306.2015, dtype=torch.float64),\n tensor(306.1598, dtype=torch.float64),\n tensor(306.1180, dtype=torch.float64),\n tensor(306.0763, dtype=torch.float64),\n tensor(306.0347, dtype=torch.float64),\n tensor(305.9930, dtype=torch.float64),\n tensor(305.9514, dtype=torch.float64),\n tensor(305.9099, dtype=torch.float64),\n tensor(305.8684, dtype=torch.float64),\n tensor(305.8269, dtype=torch.float64),\n tensor(305.7855, dtype=torch.float64),\n tensor(305.7441, dtype=torch.float64),\n tensor(305.7027, dtype=torch.float64),\n tensor(305.6614, dtype=torch.float64),\n tensor(305.6201, dtype=torch.float64),\n tensor(305.5788, dtype=torch.float64),\n tensor(305.5376, dtype=torch.float64),\n tensor(305.4964, dtype=torch.float64),\n tensor(305.4553, dtype=torch.float64),\n tensor(305.4142, dtype=torch.float64),\n tensor(305.3731, dtype=torch.float64),\n tensor(305.3321, dtype=torch.float64),\n tensor(305.2911, dtype=torch.float64),\n tensor(305.2502, dtype=torch.float64),\n tensor(305.2092, dtype=torch.float64),\n tensor(305.1684, dtype=torch.float64),\n tensor(305.1275, dtype=torch.float64),\n tensor(305.0867, dtype=torch.float64),\n tensor(305.0459, dtype=torch.float64),\n tensor(305.0052, dtype=torch.float64),\n tensor(304.9645, dtype=torch.float64),\n tensor(304.9238, dtype=torch.float64),\n tensor(304.8832, dtype=torch.float64),\n tensor(304.8426, dtype=torch.float64),\n tensor(304.8020, dtype=torch.float64),\n tensor(304.7615, dtype=torch.float64),\n tensor(304.7210, dtype=torch.float64),\n tensor(304.6806, dtype=torch.float64),\n tensor(304.6402, dtype=torch.float64),\n tensor(304.5998, dtype=torch.float64),\n tensor(304.5595, dtype=torch.float64),\n tensor(304.5192, dtype=torch.float64),\n tensor(304.4789, dtype=torch.float64),\n tensor(304.4387, dtype=torch.float64),\n tensor(304.3985, dtype=torch.float64),\n tensor(304.3583, dtype=torch.float64),\n tensor(304.3182, dtype=torch.float64),\n tensor(304.2781, dtype=torch.float64),\n tensor(304.2381, dtype=torch.float64),\n tensor(304.1981, dtype=torch.float64),\n tensor(304.1581, dtype=torch.float64),\n tensor(304.1181, dtype=torch.float64),\n tensor(304.0782, dtype=torch.float64),\n tensor(304.0384, dtype=torch.float64),\n tensor(303.9985, dtype=torch.float64),\n tensor(303.9587, dtype=torch.float64),\n tensor(303.9190, dtype=torch.float64),\n tensor(303.8792, dtype=torch.float64),\n tensor(303.8395, dtype=torch.float64),\n tensor(303.7999, dtype=torch.float64),\n tensor(303.7603, dtype=torch.float64),\n tensor(303.7207, dtype=torch.float64),\n tensor(303.6811, dtype=torch.float64),\n tensor(303.6416, dtype=torch.float64),\n tensor(303.6021, dtype=torch.float64),\n tensor(303.5627, dtype=torch.float64),\n tensor(303.5232, dtype=torch.float64),\n tensor(303.4839, dtype=torch.float64),\n tensor(303.4445, dtype=torch.float64),\n tensor(303.4052, dtype=torch.float64),\n tensor(303.3659, dtype=torch.float64),\n tensor(303.3267, dtype=torch.float64),\n tensor(303.2875, dtype=torch.float64),\n tensor(303.2483, dtype=torch.float64),\n tensor(303.2092, dtype=torch.float64),\n tensor(303.1701, dtype=torch.float64),\n tensor(303.1310, dtype=torch.float64),\n tensor(303.0920, dtype=torch.float64),\n tensor(303.0530, dtype=torch.float64),\n tensor(303.0140, dtype=torch.float64),\n tensor(302.9751, dtype=torch.float64),\n tensor(302.9362, dtype=torch.float64),\n tensor(302.8974, dtype=torch.float64),\n tensor(302.8585, dtype=torch.float64),\n tensor(302.8198, dtype=torch.float64),\n tensor(302.7810, dtype=torch.float64),\n tensor(302.7423, dtype=torch.float64),\n tensor(302.7036, dtype=torch.float64),\n tensor(302.6649, dtype=torch.float64),\n tensor(302.6263, dtype=torch.float64),\n tensor(302.5877, dtype=torch.float64),\n tensor(302.5492, dtype=torch.float64),\n tensor(302.5107, dtype=torch.float64),\n tensor(302.4722, dtype=torch.float64),\n tensor(302.4337, dtype=torch.float64),\n tensor(302.3953, dtype=torch.float64),\n tensor(302.3569, dtype=torch.float64),\n tensor(302.3186, dtype=torch.float64),\n tensor(302.2803, dtype=torch.float64),\n tensor(302.2420, dtype=torch.float64),\n tensor(302.2038, dtype=torch.float64),\n tensor(302.1656, dtype=torch.float64),\n tensor(302.1274, dtype=torch.float64),\n tensor(302.0892, dtype=torch.float64),\n tensor(302.0511, dtype=torch.float64),\n tensor(302.0130, dtype=torch.float64),\n tensor(301.9750, dtype=torch.float64),\n tensor(301.9370, dtype=torch.float64),\n tensor(301.8990, dtype=torch.float64),\n tensor(301.8611, dtype=torch.float64),\n tensor(301.8232, dtype=torch.float64),\n tensor(301.7853, dtype=torch.float64),\n tensor(301.7474, dtype=torch.float64),\n tensor(301.7096, dtype=torch.float64),\n tensor(301.6719, dtype=torch.float64),\n tensor(301.6341, dtype=torch.float64),\n tensor(301.5964, dtype=torch.float64),\n tensor(301.5587, dtype=torch.float64),\n tensor(301.5211, dtype=torch.float64),\n tensor(301.4835, dtype=torch.float64),\n tensor(301.4459, dtype=torch.float64),\n tensor(301.4083, dtype=torch.float64),\n tensor(301.3708, dtype=torch.float64),\n tensor(301.3333, dtype=torch.float64),\n tensor(301.2959, dtype=torch.float64),\n tensor(301.2585, dtype=torch.float64),\n tensor(301.2211, dtype=torch.float64),\n tensor(301.1838, dtype=torch.float64),\n tensor(301.1464, dtype=torch.float64),\n tensor(301.1092, dtype=torch.float64),\n tensor(301.0719, dtype=torch.float64),\n tensor(301.0347, dtype=torch.float64),\n tensor(300.9975, dtype=torch.float64),\n tensor(300.9603, dtype=torch.float64),\n tensor(300.9232, dtype=torch.float64),\n tensor(300.8861, dtype=torch.float64),\n tensor(300.8491, dtype=torch.float64),\n tensor(300.8121, dtype=torch.float64),\n tensor(300.7751, dtype=torch.float64),\n tensor(300.7381, dtype=torch.float64),\n tensor(300.7012, dtype=torch.float64),\n tensor(300.6643, dtype=torch.float64),\n tensor(300.6274, dtype=torch.float64),\n tensor(300.5906, dtype=torch.float64),\n tensor(300.5538, dtype=torch.float64),\n tensor(300.5170, dtype=torch.float64),\n tensor(300.4803, dtype=torch.float64),\n tensor(300.4436, dtype=torch.float64),\n tensor(300.4069, dtype=torch.float64),\n tensor(300.3703, dtype=torch.float64),\n tensor(300.3337, dtype=torch.float64),\n tensor(300.2971, dtype=torch.float64),\n tensor(300.2606, dtype=torch.float64),\n tensor(300.2241, dtype=torch.float64),\n tensor(300.1876, dtype=torch.float64),\n tensor(300.1511, dtype=torch.float64),\n tensor(300.1147, dtype=torch.float64),\n tensor(300.0783, dtype=torch.float64),\n tensor(300.0420, dtype=torch.float64),\n tensor(300.0057, dtype=torch.float64),\n tensor(299.9694, dtype=torch.float64),\n tensor(299.9331, dtype=torch.float64),\n tensor(299.8969, dtype=torch.float64),\n tensor(299.8607, dtype=torch.float64),\n tensor(299.8245, dtype=torch.float64),\n tensor(299.7884, dtype=torch.float64),\n tensor(299.7523, dtype=torch.float64),\n tensor(299.7162, dtype=torch.float64),\n tensor(299.6802, dtype=torch.float64),\n tensor(299.6442, dtype=torch.float64),\n tensor(299.6082, dtype=torch.float64),\n tensor(299.5723, dtype=torch.float64),\n tensor(299.5363, dtype=torch.float64),\n tensor(299.5005, dtype=torch.float64),\n tensor(299.4646, dtype=torch.float64),\n tensor(299.4288, dtype=torch.float64),\n tensor(299.3930, dtype=torch.float64),\n tensor(299.3572, dtype=torch.float64),\n tensor(299.3215, dtype=torch.float64),\n tensor(299.2858, dtype=torch.float64),\n tensor(299.2502, dtype=torch.float64),\n tensor(299.2145, dtype=torch.float64),\n tensor(299.1789, dtype=torch.float64),\n tensor(299.1434, dtype=torch.float64),\n tensor(299.1078, dtype=torch.float64),\n tensor(299.0723, dtype=torch.float64),\n tensor(299.0368, dtype=torch.float64),\n tensor(299.0014, dtype=torch.float64),\n tensor(298.9659, dtype=torch.float64),\n tensor(298.9306, dtype=torch.float64),\n tensor(298.8952, dtype=torch.float64),\n tensor(298.8599, dtype=torch.float64),\n tensor(298.8246, dtype=torch.float64),\n tensor(298.7893, dtype=torch.float64),\n tensor(298.7541, dtype=torch.float64),\n tensor(298.7189, dtype=torch.float64),\n tensor(298.6837, dtype=torch.float64),\n tensor(298.6485, dtype=torch.float64),\n tensor(298.6134, dtype=torch.float64),\n tensor(298.5783, dtype=torch.float64),\n tensor(298.5433, dtype=torch.float64),\n tensor(298.5083, dtype=torch.float64),\n tensor(298.4733, dtype=torch.float64),\n tensor(298.4383, dtype=torch.float64),\n tensor(298.4034, dtype=torch.float64),\n tensor(298.3685, dtype=torch.float64),\n tensor(298.3336, dtype=torch.float64),\n tensor(298.2988, dtype=torch.float64),\n tensor(298.2639, dtype=torch.float64),\n tensor(298.2292, dtype=torch.float64),\n tensor(298.1944, dtype=torch.float64),\n tensor(298.1597, dtype=torch.float64),\n tensor(298.1250, dtype=torch.float64),\n tensor(298.0903, dtype=torch.float64),\n tensor(298.0557, dtype=torch.float64),\n tensor(298.0211, dtype=torch.float64),\n tensor(297.9865, dtype=torch.float64),\n tensor(297.9520, dtype=torch.float64),\n tensor(297.9174, dtype=torch.float64),\n tensor(297.8830, dtype=torch.float64),\n tensor(297.8485, dtype=torch.float64),\n tensor(297.8141, dtype=torch.float64),\n tensor(297.7797, dtype=torch.float64),\n tensor(297.7453, dtype=torch.float64),\n tensor(297.7110, dtype=torch.float64),\n tensor(297.6767, dtype=torch.float64),\n tensor(297.6424, dtype=torch.float64),\n tensor(297.6081, dtype=torch.float64),\n tensor(297.5739, dtype=torch.float64),\n tensor(297.5397, dtype=torch.float64),\n tensor(297.5056, dtype=torch.float64),\n tensor(297.4714, dtype=torch.float64),\n tensor(297.4373, dtype=torch.float64),\n tensor(297.4032, dtype=torch.float64),\n tensor(297.3692, dtype=torch.float64),\n tensor(297.3352, dtype=torch.float64),\n tensor(297.3012, dtype=torch.float64),\n tensor(297.2672, dtype=torch.float64),\n tensor(297.2333, dtype=torch.float64),\n tensor(297.1994, dtype=torch.float64),\n tensor(297.1655, dtype=torch.float64),\n tensor(297.1317, dtype=torch.float64),\n tensor(297.0979, dtype=torch.float64),\n tensor(297.0641, dtype=torch.float64),\n tensor(297.0303, dtype=torch.float64),\n tensor(296.9966, dtype=torch.float64),\n tensor(296.9629, dtype=torch.float64),\n tensor(296.9292, dtype=torch.float64),\n tensor(296.8956, dtype=torch.float64),\n tensor(296.8620, dtype=torch.float64),\n tensor(296.8284, dtype=torch.float64),\n tensor(296.7948, dtype=torch.float64),\n tensor(296.7613, dtype=torch.float64),\n tensor(296.7278, dtype=torch.float64),\n tensor(296.6943, dtype=torch.float64),\n tensor(296.6609, dtype=torch.float64),\n tensor(296.6275, dtype=torch.float64),\n tensor(296.5941, dtype=torch.float64),\n tensor(296.5608, dtype=torch.float64),\n tensor(296.5274, dtype=torch.float64),\n tensor(296.4941, dtype=torch.float64),\n tensor(296.4609, dtype=torch.float64),\n tensor(296.4276, dtype=torch.float64),\n tensor(296.3944, dtype=torch.float64),\n tensor(296.3612, dtype=torch.float64),\n tensor(296.3281, dtype=torch.float64),\n tensor(296.2949, dtype=torch.float64),\n tensor(296.2618, dtype=torch.float64),\n tensor(296.2288, dtype=torch.float64),\n tensor(296.1957, dtype=torch.float64),\n tensor(296.1627, dtype=torch.float64),\n tensor(296.1297, dtype=torch.float64),\n tensor(296.0967, dtype=torch.float64),\n tensor(296.0638, dtype=torch.float64),\n tensor(296.0309, dtype=torch.float64),\n tensor(295.9980, dtype=torch.float64),\n tensor(295.9652, dtype=torch.float64),\n tensor(295.9324, dtype=torch.float64),\n tensor(295.8996, dtype=torch.float64),\n tensor(295.8668, dtype=torch.float64),\n tensor(295.8341, dtype=torch.float64),\n tensor(295.8013, dtype=torch.float64),\n tensor(295.7687, dtype=torch.float64),\n tensor(295.7360, dtype=torch.float64),\n tensor(295.7034, dtype=torch.float64),\n tensor(295.6708, dtype=torch.float64),\n tensor(295.6382, dtype=torch.float64),\n tensor(295.6057, dtype=torch.float64),\n tensor(295.5731, dtype=torch.float64),\n tensor(295.5407, dtype=torch.float64),\n tensor(295.5082, dtype=torch.float64),\n tensor(295.4758, dtype=torch.float64),\n tensor(295.4434, dtype=torch.float64),\n tensor(295.4110, dtype=torch.float64),\n tensor(295.3786, dtype=torch.float64),\n tensor(295.3463, dtype=torch.float64),\n tensor(295.3140, dtype=torch.float64),\n tensor(295.2817, dtype=torch.float64),\n tensor(295.2495, dtype=torch.float64),\n tensor(295.2173, dtype=torch.float64),\n tensor(295.1851, dtype=torch.float64),\n tensor(295.1529, dtype=torch.float64),\n tensor(295.1208, dtype=torch.float64),\n tensor(295.0887, dtype=torch.float64),\n tensor(295.0566, dtype=torch.float64),\n tensor(295.0245, dtype=torch.float64),\n tensor(294.9925, dtype=torch.float64),\n tensor(294.9605, dtype=torch.float64),\n tensor(294.9285, dtype=torch.float64),\n tensor(294.8966, dtype=torch.float64),\n tensor(294.8647, dtype=torch.float64),\n tensor(294.8328, dtype=torch.float64),\n tensor(294.8009, dtype=torch.float64),\n tensor(294.7691, dtype=torch.float64),\n tensor(294.7373, dtype=torch.float64),\n tensor(294.7055, dtype=torch.float64),\n tensor(294.6737, dtype=torch.float64),\n tensor(294.6420, dtype=torch.float64),\n tensor(294.6103, dtype=torch.float64),\n tensor(294.5786, dtype=torch.float64),\n tensor(294.5470, dtype=torch.float64),\n tensor(294.5154, dtype=torch.float64),\n tensor(294.4838, dtype=torch.float64),\n tensor(294.4522, dtype=torch.float64),\n tensor(294.4206, dtype=torch.float64),\n tensor(294.3891, dtype=torch.float64),\n tensor(294.3576, dtype=torch.float64),\n tensor(294.3262, dtype=torch.float64),\n tensor(294.2947, dtype=torch.float64),\n tensor(294.2633, dtype=torch.float64),\n tensor(294.2319, dtype=torch.float64),\n tensor(294.2006, dtype=torch.float64),\n tensor(294.1692, dtype=torch.float64),\n tensor(294.1379, dtype=torch.float64),\n tensor(294.1067, dtype=torch.float64),\n tensor(294.0754, dtype=torch.float64),\n tensor(294.0442, dtype=torch.float64),\n tensor(294.0130, dtype=torch.float64),\n tensor(293.9818, dtype=torch.float64),\n tensor(293.9506, dtype=torch.float64),\n tensor(293.9195, dtype=torch.float64),\n tensor(293.8884, dtype=torch.float64),\n tensor(293.8574, dtype=torch.float64),\n tensor(293.8263, dtype=torch.float64),\n tensor(293.7953, dtype=torch.float64),\n tensor(293.7643, dtype=torch.float64),\n tensor(293.7333, dtype=torch.float64),\n tensor(293.7024, dtype=torch.float64),\n tensor(293.6715, dtype=torch.float64),\n tensor(293.6406, dtype=torch.float64),\n tensor(293.6097, dtype=torch.float64),\n tensor(293.5789, dtype=torch.float64),\n tensor(293.5481, dtype=torch.float64),\n tensor(293.5173, dtype=torch.float64),\n tensor(293.4865, dtype=torch.float64),\n tensor(293.4558, dtype=torch.float64),\n tensor(293.4251, dtype=torch.float64),\n tensor(293.3944, dtype=torch.float64),\n tensor(293.3637, dtype=torch.float64),\n tensor(293.3331, dtype=torch.float64),\n tensor(293.3025, dtype=torch.float64),\n tensor(293.2719, dtype=torch.float64),\n tensor(293.2414, dtype=torch.float64),\n tensor(293.2108, dtype=torch.float64),\n tensor(293.1803, dtype=torch.float64),\n tensor(293.1498, dtype=torch.float64),\n tensor(293.1194, dtype=torch.float64),\n tensor(293.0890, dtype=torch.float64),\n tensor(293.0586, dtype=torch.float64),\n tensor(293.0282, dtype=torch.float64),\n tensor(292.9978, dtype=torch.float64),\n tensor(292.9675, dtype=torch.float64),\n tensor(292.9372, dtype=torch.float64),\n tensor(292.9069, dtype=torch.float64),\n tensor(292.8767, dtype=torch.float64),\n tensor(292.8464, dtype=torch.float64),\n tensor(292.8162, dtype=torch.float64),\n tensor(292.7861, dtype=torch.float64),\n tensor(292.7559, dtype=torch.float64),\n tensor(292.7258, dtype=torch.float64),\n tensor(292.6957, dtype=torch.float64),\n tensor(292.6656, dtype=torch.float64),\n tensor(292.6356, dtype=torch.float64),\n tensor(292.6055, dtype=torch.float64),\n tensor(292.5755, dtype=torch.float64),\n tensor(292.5456, dtype=torch.float64),\n tensor(292.5156, dtype=torch.float64),\n tensor(292.4857, dtype=torch.float64),\n tensor(292.4558, dtype=torch.float64),\n tensor(292.4259, dtype=torch.float64),\n tensor(292.3961, dtype=torch.float64),\n tensor(292.3662, dtype=torch.float64),\n tensor(292.3364, dtype=torch.float64),\n tensor(292.3067, dtype=torch.float64),\n tensor(292.2769, dtype=torch.float64),\n tensor(292.2472, dtype=torch.float64),\n tensor(292.2175, dtype=torch.float64),\n tensor(292.1878, dtype=torch.float64),\n tensor(292.1581, dtype=torch.float64),\n tensor(292.1285, dtype=torch.float64),\n tensor(292.0989, dtype=torch.float64),\n tensor(292.0693, dtype=torch.float64),\n tensor(292.0398, dtype=torch.float64),\n tensor(292.0102, dtype=torch.float64),\n tensor(291.9807, dtype=torch.float64),\n tensor(291.9512, dtype=torch.float64),\n tensor(291.9218, dtype=torch.float64),\n tensor(291.8924, dtype=torch.float64),\n tensor(291.8629, dtype=torch.float64),\n tensor(291.8336, dtype=torch.float64),\n tensor(291.8042, dtype=torch.float64),\n tensor(291.7749, dtype=torch.float64),\n tensor(291.7456, dtype=torch.float64),\n tensor(291.7163, dtype=torch.float64),\n tensor(291.6870, dtype=torch.float64),\n tensor(291.6578, dtype=torch.float64),\n tensor(291.6286, dtype=torch.float64),\n tensor(291.5994, dtype=torch.float64),\n tensor(291.5702, dtype=torch.float64),\n tensor(291.5411, dtype=torch.float64),\n tensor(291.5119, dtype=torch.float64),\n tensor(291.4828, dtype=torch.float64),\n tensor(291.4538, dtype=torch.float64),\n tensor(291.4247, dtype=torch.float64),\n tensor(291.3957, dtype=torch.float64),\n tensor(291.3667, dtype=torch.float64),\n tensor(291.3377, dtype=torch.float64),\n tensor(291.3088, dtype=torch.float64),\n tensor(291.2799, dtype=torch.float64),\n tensor(291.2510, dtype=torch.float64),\n tensor(291.2221, dtype=torch.float64),\n tensor(291.1932, dtype=torch.float64),\n tensor(291.1644, dtype=torch.float64),\n tensor(291.1356, dtype=torch.float64),\n tensor(291.1068, dtype=torch.float64),\n tensor(291.0780, dtype=torch.float64),\n tensor(291.0493, dtype=torch.float64),\n tensor(291.0206, dtype=torch.float64),\n tensor(290.9919, dtype=torch.float64),\n tensor(290.9632, dtype=torch.float64),\n tensor(290.9346, dtype=torch.float64),\n tensor(290.9060, dtype=torch.float64),\n tensor(290.8774, dtype=torch.float64),\n tensor(290.8488, dtype=torch.float64),\n tensor(290.8203, dtype=torch.float64),\n tensor(290.7917, dtype=torch.float64),\n tensor(290.7632, dtype=torch.float64),\n tensor(290.7348, dtype=torch.float64),\n tensor(290.7063, dtype=torch.float64),\n tensor(290.6779, dtype=torch.float64),\n tensor(290.6495, dtype=torch.float64),\n tensor(290.6211, dtype=torch.float64),\n tensor(290.5927, dtype=torch.float64),\n tensor(290.5644, dtype=torch.float64),\n tensor(290.5361, dtype=torch.float64),\n tensor(290.5078, dtype=torch.float64),\n tensor(290.4795, dtype=torch.float64),\n tensor(290.4513, dtype=torch.float64),\n tensor(290.4231, dtype=torch.float64),\n tensor(290.3949, dtype=torch.float64),\n tensor(290.3667, dtype=torch.float64),\n tensor(290.3385, dtype=torch.float64),\n tensor(290.3104, dtype=torch.float64),\n tensor(290.2823, dtype=torch.float64),\n tensor(290.2542, dtype=torch.float64),\n tensor(290.2262, dtype=torch.float64),\n tensor(290.1981, dtype=torch.float64),\n tensor(290.1701, dtype=torch.float64),\n tensor(290.1421, dtype=torch.float64),\n tensor(290.1142, dtype=torch.float64),\n tensor(290.0862, dtype=torch.float64),\n tensor(290.0583, dtype=torch.float64),\n tensor(290.0304, dtype=torch.float64),\n tensor(290.0025, dtype=torch.float64),\n tensor(289.9747, dtype=torch.float64),\n tensor(289.9469, dtype=torch.float64),\n tensor(289.9191, dtype=torch.float64),\n tensor(289.8913, dtype=torch.float64),\n tensor(289.8635, dtype=torch.float64),\n tensor(289.8358, dtype=torch.float64),\n tensor(289.8081, dtype=torch.float64),\n tensor(289.7804, dtype=torch.float64),\n tensor(289.7527, dtype=torch.float64),\n tensor(289.7250, dtype=torch.float64),\n tensor(289.6974, dtype=torch.float64),\n tensor(289.6698, dtype=torch.float64),\n tensor(289.6422, dtype=torch.float64),\n tensor(289.6147, dtype=torch.float64),\n tensor(289.5871, dtype=torch.float64),\n tensor(289.5596, dtype=torch.float64),\n tensor(289.5321, dtype=torch.float64),\n tensor(289.5047, dtype=torch.float64),\n tensor(289.4772, dtype=torch.float64),\n tensor(289.4498, dtype=torch.float64),\n tensor(289.4224, dtype=torch.float64),\n tensor(289.3950, dtype=torch.float64),\n tensor(289.3677, dtype=torch.float64),\n tensor(289.3404, dtype=torch.float64),\n tensor(289.3130, dtype=torch.float64),\n tensor(289.2858, dtype=torch.float64),\n tensor(289.2585, dtype=torch.float64),\n tensor(289.2312, dtype=torch.float64),\n tensor(289.2040, dtype=torch.float64),\n tensor(289.1768, dtype=torch.float64),\n tensor(289.1496, dtype=torch.float64),\n tensor(289.1225, dtype=torch.float64),\n tensor(289.0954, dtype=torch.float64),\n tensor(289.0683, dtype=torch.float64),\n tensor(289.0412, dtype=torch.float64),\n tensor(289.0141, dtype=torch.float64),\n tensor(288.9871, dtype=torch.float64),\n tensor(288.9600, dtype=torch.float64),\n tensor(288.9330, dtype=torch.float64),\n tensor(288.9061, dtype=torch.float64),\n tensor(288.8791, dtype=torch.float64),\n tensor(288.8522, dtype=torch.float64),\n tensor(288.8253, dtype=torch.float64),\n tensor(288.7984, dtype=torch.float64),\n tensor(288.7715, dtype=torch.float64),\n tensor(288.7446, dtype=torch.float64),\n tensor(288.7178, dtype=torch.float64),\n tensor(288.6910, dtype=torch.float64),\n tensor(288.6642, dtype=torch.float64),\n tensor(288.6375, dtype=torch.float64),\n tensor(288.6107, dtype=torch.float64),\n tensor(288.5840, dtype=torch.float64),\n tensor(288.5573, dtype=torch.float64),\n tensor(288.5307, dtype=torch.float64),\n tensor(288.5040, dtype=torch.float64),\n tensor(288.4774, dtype=torch.float64),\n tensor(288.4508, dtype=torch.float64),\n tensor(288.4242, dtype=torch.float64),\n tensor(288.3976, dtype=torch.float64),\n tensor(288.3711, dtype=torch.float64),\n tensor(288.3446, dtype=torch.float64),\n tensor(288.3181, dtype=torch.float64),\n tensor(288.2916, dtype=torch.float64),\n tensor(288.2651, dtype=torch.float64),\n tensor(288.2387, dtype=torch.float64),\n tensor(288.2123, dtype=torch.float64),\n tensor(288.1859, dtype=torch.float64),\n tensor(288.1595, dtype=torch.float64),\n tensor(288.1332, dtype=torch.float64),\n tensor(288.1068, dtype=torch.float64),\n tensor(288.0805, dtype=torch.float64),\n tensor(288.0543, dtype=torch.float64),\n tensor(288.0280, dtype=torch.float64),\n tensor(288.0018, dtype=torch.float64),\n tensor(287.9755, dtype=torch.float64),\n tensor(287.9493, dtype=torch.float64),\n tensor(287.9232, dtype=torch.float64),\n tensor(287.8970, dtype=torch.float64),\n tensor(287.8709, dtype=torch.float64),\n tensor(287.8447, dtype=torch.float64),\n tensor(287.8187, dtype=torch.float64),\n tensor(287.7926, dtype=torch.float64),\n tensor(287.7665, dtype=torch.float64),\n tensor(287.7405, dtype=torch.float64),\n tensor(287.7145, dtype=torch.float64),\n tensor(287.6885, dtype=torch.float64),\n tensor(287.6625, dtype=torch.float64),\n tensor(287.6366, dtype=torch.float64),\n tensor(287.6107, dtype=torch.float64),\n tensor(287.5848, dtype=torch.float64),\n tensor(287.5589, dtype=torch.float64),\n tensor(287.5330, dtype=torch.float64),\n tensor(287.5072, dtype=torch.float64),\n tensor(287.4814, dtype=torch.float64),\n tensor(287.4556, dtype=torch.float64),\n tensor(287.4298, dtype=torch.float64),\n tensor(287.4040, dtype=torch.float64),\n tensor(287.3783, dtype=torch.float64),\n tensor(287.3526, dtype=torch.float64),\n tensor(287.3269, dtype=torch.float64),\n tensor(287.3012, dtype=torch.float64),\n tensor(287.2755, dtype=torch.float64),\n tensor(287.2499, dtype=torch.float64),\n tensor(287.2243, dtype=torch.float64),\n tensor(287.1987, dtype=torch.float64),\n tensor(287.1731, dtype=torch.float64),\n tensor(287.1476, dtype=torch.float64),\n tensor(287.1221, dtype=torch.float64),\n tensor(287.0965, dtype=torch.float64),\n tensor(287.0711, dtype=torch.float64),\n tensor(287.0456, dtype=torch.float64),\n tensor(287.0201, dtype=torch.float64),\n tensor(286.9947, dtype=torch.float64),\n tensor(286.9693, dtype=torch.float64),\n tensor(286.9439, dtype=torch.float64),\n tensor(286.9186, dtype=torch.float64),\n tensor(286.8932, dtype=torch.float64),\n tensor(286.8679, dtype=torch.float64),\n tensor(286.8426, dtype=torch.float64),\n tensor(286.8173, dtype=torch.float64),\n tensor(286.7920, dtype=torch.float64),\n tensor(286.7668, dtype=torch.float64),\n tensor(286.7416, dtype=torch.float64),\n tensor(286.7164, dtype=torch.float64),\n tensor(286.6912, dtype=torch.float64),\n tensor(286.6660, dtype=torch.float64),\n tensor(286.6409, dtype=torch.float64),\n tensor(286.6158, dtype=torch.float64),\n tensor(286.5907, dtype=torch.float64),\n tensor(286.5656, dtype=torch.float64),\n tensor(286.5405, dtype=torch.float64),\n tensor(286.5155, dtype=torch.float64),\n tensor(286.4905, dtype=torch.float64),\n tensor(286.4655, dtype=torch.float64),\n tensor(286.4405, dtype=torch.float64),\n tensor(286.4155, dtype=torch.float64),\n tensor(286.3906, dtype=torch.float64),\n tensor(286.3657, dtype=torch.float64),\n tensor(286.3408, dtype=torch.float64),\n tensor(286.3159, dtype=torch.float64),\n tensor(286.2910, dtype=torch.float64),\n tensor(286.2662, dtype=torch.float64),\n tensor(286.2414, dtype=torch.float64),\n tensor(286.2166, dtype=torch.float64),\n tensor(286.1918, dtype=torch.float64),\n tensor(286.1670, dtype=torch.float64),\n tensor(286.1423, dtype=torch.float64),\n tensor(286.1176, dtype=torch.float64),\n tensor(286.0929, dtype=torch.float64),\n tensor(286.0682, dtype=torch.float64),\n tensor(286.0435, dtype=torch.float64),\n tensor(286.0189, dtype=torch.float64),\n tensor(285.9942, dtype=torch.float64),\n tensor(285.9696, dtype=torch.float64),\n tensor(285.9451, dtype=torch.float64),\n tensor(285.9205, dtype=torch.float64),\n tensor(285.8960, dtype=torch.float64),\n tensor(285.8714, dtype=torch.float64),\n tensor(285.8469, dtype=torch.float64),\n tensor(285.8225, dtype=torch.float64),\n tensor(285.7980, dtype=torch.float64),\n tensor(285.7735, dtype=torch.float64),\n tensor(285.7491, dtype=torch.float64),\n tensor(285.7247, dtype=torch.float64),\n tensor(285.7003, dtype=torch.float64),\n tensor(285.6760, dtype=torch.float64),\n tensor(285.6516, dtype=torch.float64),\n tensor(285.6273, dtype=torch.float64),\n tensor(285.6030, dtype=torch.float64),\n tensor(285.5787, dtype=torch.float64),\n tensor(285.5544, dtype=torch.float64),\n tensor(285.5302, dtype=torch.float64),\n tensor(285.5059, dtype=torch.float64),\n tensor(285.4817, dtype=torch.float64),\n tensor(285.4575, dtype=torch.float64),\n tensor(285.4334, dtype=torch.float64),\n tensor(285.4092, dtype=torch.float64),\n tensor(285.3851, dtype=torch.float64),\n tensor(285.3610, dtype=torch.float64),\n tensor(285.3369, dtype=torch.float64),\n tensor(285.3128, dtype=torch.float64),\n tensor(285.2887, dtype=torch.float64),\n tensor(285.2647, dtype=torch.float64),\n tensor(285.2407, dtype=torch.float64),\n tensor(285.2167, dtype=torch.float64),\n tensor(285.1927, dtype=torch.float64),\n tensor(285.1688, dtype=torch.float64),\n tensor(285.1448, dtype=torch.float64),\n tensor(285.1209, dtype=torch.float64),\n tensor(285.0970, dtype=torch.float64),\n tensor(285.0731, dtype=torch.float64),\n tensor(285.0492, dtype=torch.float64),\n tensor(285.0254, dtype=torch.float64),\n tensor(285.0016, dtype=torch.float64),\n tensor(284.9778, dtype=torch.float64),\n tensor(284.9540, dtype=torch.float64),\n tensor(284.9302, dtype=torch.float64),\n tensor(284.9065, dtype=torch.float64),\n tensor(284.8827, dtype=torch.float64),\n tensor(284.8590, dtype=torch.float64),\n tensor(284.8353, dtype=torch.float64),\n tensor(284.8116, dtype=torch.float64),\n tensor(284.7880, dtype=torch.float64),\n tensor(284.7644, dtype=torch.float64),\n tensor(284.7407, dtype=torch.float64),\n tensor(284.7171, dtype=torch.float64),\n tensor(284.6936, dtype=torch.float64),\n tensor(284.6700, dtype=torch.float64),\n tensor(284.6465, dtype=torch.float64),\n tensor(284.6229, dtype=torch.float64),\n tensor(284.5994, dtype=torch.float64),\n tensor(284.5759, dtype=torch.float64),\n tensor(284.5525, dtype=torch.float64),\n tensor(284.5290, dtype=torch.float64),\n tensor(284.5056, dtype=torch.float64),\n tensor(284.4822, dtype=torch.float64),\n tensor(284.4588, dtype=torch.float64),\n tensor(284.4354, dtype=torch.float64),\n tensor(284.4121, dtype=torch.float64),\n tensor(284.3887, dtype=torch.float64),\n tensor(284.3654, dtype=torch.float64),\n tensor(284.3421, dtype=torch.float64),\n tensor(284.3188, dtype=torch.float64),\n tensor(284.2956, dtype=torch.float64),\n tensor(284.2723, dtype=torch.float64),\n tensor(284.2491, dtype=torch.float64),\n tensor(284.2259, dtype=torch.float64),\n tensor(284.2027, dtype=torch.float64),\n tensor(284.1795, dtype=torch.float64),\n tensor(284.1564, dtype=torch.float64),\n tensor(284.1332, dtype=torch.float64),\n tensor(284.1101, dtype=torch.float64),\n tensor(284.0870, dtype=torch.float64),\n tensor(284.0639, dtype=torch.float64),\n tensor(284.0409, dtype=torch.float64),\n tensor(284.0178, dtype=torch.float64),\n tensor(283.9948, dtype=torch.float64),\n tensor(283.9718, dtype=torch.float64),\n tensor(283.9488, dtype=torch.float64),\n tensor(283.9259, dtype=torch.float64),\n tensor(283.9029, dtype=torch.float64),\n tensor(283.8800, dtype=torch.float64),\n tensor(283.8571, dtype=torch.float64),\n tensor(283.8342, dtype=torch.float64),\n tensor(283.8113, dtype=torch.float64),\n tensor(283.7884, dtype=torch.float64),\n tensor(283.7656, dtype=torch.float64),\n tensor(283.7428, dtype=torch.float64),\n tensor(283.7200, dtype=torch.float64),\n tensor(283.6972, dtype=torch.float64),\n tensor(283.6744, dtype=torch.float64),\n tensor(283.6517, dtype=torch.float64),\n tensor(283.6289, dtype=torch.float64),\n tensor(283.6062, dtype=torch.float64),\n tensor(283.5835, dtype=torch.float64),\n tensor(283.5608, dtype=torch.float64),\n tensor(283.5382, dtype=torch.float64),\n tensor(283.5155, dtype=torch.float64),\n tensor(283.4929, dtype=torch.float64),\n tensor(283.4703, dtype=torch.float64),\n tensor(283.4477, dtype=torch.float64),\n tensor(283.4251, dtype=torch.float64),\n tensor(283.4026, dtype=torch.float64),\n tensor(283.3800, dtype=torch.float64),\n tensor(283.3575, dtype=torch.float64),\n tensor(283.3350, dtype=torch.float64),\n tensor(283.3125, dtype=torch.float64),\n tensor(283.2901, dtype=torch.float64),\n tensor(283.2676, dtype=torch.float64),\n tensor(283.2452, dtype=torch.float64),\n tensor(283.2228, dtype=torch.float64),\n tensor(283.2004, dtype=torch.float64),\n tensor(283.1780, dtype=torch.float64),\n tensor(283.1557, dtype=torch.float64),\n tensor(283.1333, dtype=torch.float64),\n tensor(283.1110, dtype=torch.float64),\n tensor(283.0887, dtype=torch.float64),\n tensor(283.0664, dtype=torch.float64),\n tensor(283.0441, dtype=torch.float64),\n tensor(283.0219, dtype=torch.float64),\n tensor(282.9997, dtype=torch.float64),\n tensor(282.9774, dtype=torch.float64),\n tensor(282.9552, dtype=torch.float64),\n tensor(282.9331, dtype=torch.float64),\n tensor(282.9109, dtype=torch.float64),\n tensor(282.8888, dtype=torch.float64),\n tensor(282.8666, dtype=torch.float64),\n tensor(282.8445, dtype=torch.float64),\n tensor(282.8224, dtype=torch.float64),\n tensor(282.8003, dtype=torch.float64),\n tensor(282.7783, dtype=torch.float64),\n tensor(282.7562, dtype=torch.float64),\n tensor(282.7342, dtype=torch.float64),\n tensor(282.7122, dtype=torch.float64),\n tensor(282.6902, dtype=torch.float64),\n tensor(282.6683, dtype=torch.float64),\n tensor(282.6463, dtype=torch.float64),\n tensor(282.6244, dtype=torch.float64),\n tensor(282.6024, dtype=torch.float64),\n tensor(282.5805, dtype=torch.float64),\n tensor(282.5587, dtype=torch.float64),\n tensor(282.5368, dtype=torch.float64),\n tensor(282.5149, dtype=torch.float64),\n tensor(282.4931, dtype=torch.float64),\n tensor(282.4713, dtype=torch.float64),\n tensor(282.4495, dtype=torch.float64),\n tensor(282.4277, dtype=torch.float64),\n tensor(282.4059, dtype=torch.float64),\n tensor(282.3842, dtype=torch.float64),\n tensor(282.3625, dtype=torch.float64),\n tensor(282.3407, dtype=torch.float64),\n tensor(282.3191, dtype=torch.float64),\n tensor(282.2974, dtype=torch.float64),\n tensor(282.2757, dtype=torch.float64),\n tensor(282.2541, dtype=torch.float64),\n tensor(282.2324, dtype=torch.float64),\n tensor(282.2108, dtype=torch.float64),\n tensor(282.1892, dtype=torch.float64),\n tensor(282.1677, dtype=torch.float64),\n tensor(282.1461, dtype=torch.float64),\n tensor(282.1246, dtype=torch.float64),\n tensor(282.1030, dtype=torch.float64),\n tensor(282.0815, dtype=torch.float64),\n tensor(282.0600, dtype=torch.float64),\n tensor(282.0386, dtype=torch.float64),\n tensor(282.0171, dtype=torch.float64),\n tensor(281.9957, dtype=torch.float64),\n tensor(281.9742, dtype=torch.float64),\n tensor(281.9528, dtype=torch.float64),\n tensor(281.9314, dtype=torch.float64),\n tensor(281.9101, dtype=torch.float64),\n tensor(281.8887, dtype=torch.float64),\n tensor(281.8674, dtype=torch.float64),\n tensor(281.8460, dtype=torch.float64),\n tensor(281.8247, dtype=torch.float64),\n tensor(281.8034, dtype=torch.float64),\n tensor(281.7822, dtype=torch.float64),\n tensor(281.7609, dtype=torch.float64),\n tensor(281.7397, dtype=torch.float64),\n tensor(281.7184, dtype=torch.float64),\n tensor(281.6972, dtype=torch.float64),\n tensor(281.6760, dtype=torch.float64),\n tensor(281.6549, dtype=torch.float64),\n tensor(281.6337, dtype=torch.float64),\n tensor(281.6126, dtype=torch.float64),\n tensor(281.5915, dtype=torch.float64),\n tensor(281.5703, dtype=torch.float64),\n tensor(281.5493, dtype=torch.float64),\n tensor(281.5282, dtype=torch.float64),\n tensor(281.5071, dtype=torch.float64),\n tensor(281.4861, dtype=torch.float64),\n tensor(281.4651, dtype=torch.float64),\n tensor(281.4441, dtype=torch.float64),\n tensor(281.4231, dtype=torch.float64),\n tensor(281.4021, dtype=torch.float64),\n tensor(281.3811, dtype=torch.float64),\n tensor(281.3602, dtype=torch.float64),\n tensor(281.3393, dtype=torch.float64),\n tensor(281.3184, dtype=torch.float64),\n tensor(281.2975, dtype=torch.float64),\n tensor(281.2766, dtype=torch.float64),\n tensor(281.2557, dtype=torch.float64),\n tensor(281.2349, dtype=torch.float64),\n tensor(281.2141, dtype=torch.float64),\n tensor(281.1933, dtype=torch.float64),\n tensor(281.1725, dtype=torch.float64),\n tensor(281.1517, dtype=torch.float64),\n tensor(281.1309, dtype=torch.float64),\n tensor(281.1102, dtype=torch.float64),\n tensor(281.0894, dtype=torch.float64),\n tensor(281.0687, dtype=torch.float64),\n tensor(281.0480, dtype=torch.float64),\n tensor(281.0274, dtype=torch.float64),\n tensor(281.0067, dtype=torch.float64),\n tensor(280.9860, dtype=torch.float64),\n tensor(280.9654, dtype=torch.float64),\n tensor(280.9448, dtype=torch.float64),\n tensor(280.9242, dtype=torch.float64),\n tensor(280.9036, dtype=torch.float64),\n tensor(280.8830, dtype=torch.float64),\n tensor(280.8625, dtype=torch.float64),\n tensor(280.8420, dtype=torch.float64),\n tensor(280.8214, dtype=torch.float64),\n tensor(280.8009, dtype=torch.float64),\n tensor(280.7805, dtype=torch.float64),\n tensor(280.7600, dtype=torch.float64),\n tensor(280.7395, dtype=torch.float64),\n tensor(280.7191, dtype=torch.float64),\n tensor(280.6987, dtype=torch.float64),\n tensor(280.6783, dtype=torch.float64),\n tensor(280.6579, dtype=torch.float64),\n tensor(280.6375, dtype=torch.float64),\n tensor(280.6171, dtype=torch.float64),\n tensor(280.5968, dtype=torch.float64),\n tensor(280.5765, dtype=torch.float64),\n tensor(280.5562, dtype=torch.float64),\n tensor(280.5359, dtype=torch.float64),\n tensor(280.5156, dtype=torch.float64),\n tensor(280.4953, dtype=torch.float64),\n tensor(280.4751, dtype=torch.float64),\n tensor(280.4548, dtype=torch.float64),\n tensor(280.4346, dtype=torch.float64),\n tensor(280.4144, dtype=torch.float64),\n tensor(280.3942, dtype=torch.float64),\n tensor(280.3741, dtype=torch.float64),\n tensor(280.3539, dtype=torch.float64),\n tensor(280.3338, dtype=torch.float64),\n tensor(280.3137, dtype=torch.float64),\n tensor(280.2936, dtype=torch.float64),\n tensor(280.2735, dtype=torch.float64),\n tensor(280.2534, dtype=torch.float64),\n tensor(280.2333, dtype=torch.float64),\n tensor(280.2133, dtype=torch.float64),\n tensor(280.1933, dtype=torch.float64),\n tensor(280.1732, dtype=torch.float64),\n tensor(280.1533, dtype=torch.float64),\n tensor(280.1333, dtype=torch.float64),\n tensor(280.1133, dtype=torch.float64),\n tensor(280.0934, dtype=torch.float64),\n tensor(280.0734, dtype=torch.float64),\n tensor(280.0535, dtype=torch.float64),\n tensor(280.0336, dtype=torch.float64),\n tensor(280.0137, dtype=torch.float64),\n tensor(279.9938, dtype=torch.float64),\n tensor(279.9740, dtype=torch.float64),\n tensor(279.9541, dtype=torch.float64),\n tensor(279.9343, dtype=torch.float64),\n ...]\n\n\n\nopt3.w\n\ntensor([-9.7637e-01, -3.8816e-02, -4.1513e-01, -7.2064e-02,  1.6028e-03,\n         4.1965e+00,  2.0960e+00], dtype=torch.float64)\n\n\n\ny_test_tensor\n\ntensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n        1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.],\n       dtype=torch.float64)\n\n\n\npredicted = opt3.predict(X_test_tensor)\npredicted\n\ntensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n        1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.])\n\n\n\nfrom sklearn.metrics import confusion_matrix\nC = confusion_matrix(y_test_tensor, predicted)\nC\n\narray([[71,  9],\n       [17, 46]])\n\n\n\n(71+46)/143\n\n0.8181818181818182"
  },
  {
    "objectID": "posts/quantifying-bias/index.html",
    "href": "posts/quantifying-bias/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "Narayanan’s Position\nAs a quantitative researcher, Narayanan acknowledges in his speech that, although he is pointing out the flaws of quantitative methodology, he recognizes its importance in the world of research. Throughout his speech, he highlights several flaws in quantitating discrimination, most of which boil down to the fact that quantitative data does not tell the whole story. His perspective is that quanititative methods are just one piece of the puzzle. They are necessary, but they are not the only way to gain knowledge. Quantitative ways of measuring bias should be used in conjunction with other forms of measurement and auditing to ensure that all perspectives are observed and the fullest picture of the situation is found (Narayanan (2022)).\n\n\nBenefits of Quantifying Bias\nIn Narayanan (2022), he discusses how the ProPublica investigation into the COMPAS algorithm was successful in part because there as a quantifiable result that could “definitively” say whether or not the COMPAS algorithm was biased. Evidently, this succeeded, in part because they were able to quantify ways in which they could see evidence of bias. It then gained a lot of attention because of its success. Narayanan posits that if they had been searching in a more qualitative way, they would have gained less attention from their study and therefore would have reached a smaller audience. Essentially, humans are more likely to believe something if there is evidence for it, and quantitative evidence is sometimes prioritized over qualitative evidence (especially in the scientific community).\n\n\nDrawbacks of Quantifying Bias\nAs outlined by Narayanan:\n• Choice of null hypothesis\n• Use of snapshot datasets\n• Use of data provided by company\n• Explaining away discrimination\n• Wrong locus of intervention\n• Objectivity illusion\n• Performativity\nNarayanan outlines seven potential pitfalls of quantifying bias in his speech (Narayanan (2022)). The first one he outlines is the choice of null hypothesis. What reserachers choose as their “null hypothesis”, or base assumption of neutrality, drastically affects whether or not we call something biased. Generally, researchers say that the null hypothesis is the absence of discrimination. This sets up studies to be biased from the start, since assuming that the base state is a lack of bias ignores the systemic bias that is present in many aspects of society. Narayanan brings up the example of diversity initiatives in schools and other programs–they focus on fixing the surface level problem without addressing the root cause (discrimination) because the base assumption is that academia is an unbiased field to begin with.\nThe second pitfall he identifies is the use of snapshot datasets.\n\n\nDiscussion and Synthesis\nI generally agree with Narayanan’s perspectives on quantitative methods. I think that they have their place, but also I think what is needed is a perspective shift–that quantitative methods are not always better/more informative than qualitative ones, and that we don’t have to be able to simulate everything perfectly mathematically. While I recognize the benefits of quantitativeness,\n\n\n\n\n\n\n\n\nReferences\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elli Sterling’s CSCI 0451 Blog",
    "section": "",
    "text": "Newtons Method and Adam\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing bias with machine learning models.\n\n\n\n\n\nMar 8, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nDesigning an algorithm to decide whether or not loan applicants should receive loans.\n\n\n\n\n\nFeb 27, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying penguins by species based on the Palmer Penguins dataset\n\n\n\n\n\nFeb 12, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\nNo matching items"
  }
]