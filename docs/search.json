[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer"
  },
  {
    "objectID": "posts/auditing_bias/index.html",
    "href": "posts/auditing_bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n60\n15.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n1\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n18.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n20\n18.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n33\n18.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(99419, 15)\n(99419,)\n(99419,)\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf\n\nHow many individuals are in the dataset?\n\nlen(df)\n\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1? ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍In ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employment ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍prediction, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍would ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍correspond ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍employed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals.\n\nprint(df[\"label\"].value_counts())\nprint(35232/(35232+44303))\n\n44.3%\nOf ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍these ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍how ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍many ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍groups?\n\ndf[\"group\"].value_counts()\n\nIn ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍each ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍what ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍individuals ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍equal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1?\n\n(df[df[\"label\"] == True][\"group\"].value_counts())/df[\"group\"].value_counts()\n\nCheck ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍intersectional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍trends ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍studying ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍target ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍broken ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍out ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chosen ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍an ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍additional ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍For ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍example, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍if ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(RAC1P) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍group, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍then ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍could ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍choose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(SEX) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍compute ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍proportion ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍positive ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍labels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍both ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sex. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍might ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍good ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍opportunity ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍use ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍visualization ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍such ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍as ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bar ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chart, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍e.g. via ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍seaborn package.\n\n(df[df[\"label\"] == True].groupby(\"group\")[\"SEX\"].value_counts())/df.groupby(\"group\")[\"SEX\"].value_counts()\n\nNext step is to make a plot where the x axis is race, y is percentage positive labels, and then the hue of the bar would be indicative of sex. From the pure numbers, we can see that group 2, which represents African Americans in the dataset, is much lower than many of the other groups.\n\nimport seaborn as sns\nsns.barplot(x = \"group\", y = , data = df, hue = \"SEX\")"
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post, we will analyze the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station in Antarctica. It contains various types of quantitative and qualitative data about the penguins observed by Gorman et al. To analyze this data, we will train a machine learning model on it, and find the best combination of features to train that model on. We will then test our model against the test dataset and determine its usability. The model we will use is a Logistic Regression model, which uses linear equations to create boundaries and categorizes based on those boundaries.\n\n\nSetup\nFirst we can load in the data as a Pandas dataframe.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nWe can then look at the dataframe we have made:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can shorten the species of the penguins to just the first word, which will make it easier to catalogue them.\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nFigure Creation\nNow that we’ve preprocessed the data a little bit, we can create a few plots to get an idea of what this data means. Below, we use seaborn to create a scatterplot and a bar plot.\n\nfig, ax = plt.subplots(2, 1, figsize = (3.5, 8))\n\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = 'Delta 13 C (o/oo)', hue = \"Species\", style = \"Species\", ax = ax[0])\np2 = sns.barplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = 'Species')\n\n\n\n\n\n\n\n\nFigure 1: A scatterplot showing flipper length against Delta 13 C for three species of penguins. We can see that there is some correlation, but at the edges the species overlap a bit.\nFigure 2: A bar graph showing body mass for each species on each island. Torgerson only contains Adelie, Dream has pretty equal body masses for Adelie and Chinstrap, and Biscoe has Gentoo penguins at a much larger size than the other penguins.\nFrom these figures, we can tell that flipper length differentiates Adelie and Gentoo penguins, and that Delta 13 C differentiates Adelie and Chinstrap penguins. However, we do get some overlap between Chinstrap and Adelie that would reduce model accuracy. The second plot shows that only Adelie penguins were observed on Torgerson, Gentoos were only observed on Biscoe, and Chinstraps were only observed on Dream. Although Adelie penguins are found on all three islands, there is enough separation that we can assume that it would be an informative factor to include.\nNext, let’s group the data into an informative table.\n\ntrain.groupby(\"Species\")[\"Flipper Length (mm)\"].mean()\n\nSpecies\nAdelie       190.084034\nChinstrap    196.000000\nGentoo       216.752577\nName: Flipper Length (mm), dtype: float64\n\n\nHere we can see that Gentoo penguins have much longer flippers on average than the other species! Chinstrap has more than Adelie, but the difference is not as significant. This could indicate that flipper length will be a useful feature for our model.\nNow that we’ve seen some of the data plotted out, we can continue the preprocessing step. The predict() function below drops unnecessary columns, rows/columns with null values, assigns species to integers rather than strings, and then splits the species into a separate data structure, y.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(train)\nx_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nDeciding on attributes\nWe now need to pick the attributes that will yield the best prediction accuracy. To do this, we will use an exhaustive approach, where we check the accuracy of a linear regression model trained on all possible feature combinations. In the code below, we loop through all possible combinations of qualitative and quantitative attributes. In each iteration, we train a model on the different combinations of features, and we compile an array of all the combinations (along with their scores and the model that was trained on them).\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Sex\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n1    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n2    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n3    new_score = LR.score(x_train[cols], y_train)\n4    scores.append((cols, new_score, LR))\n\n\n1\n\nGather the features for this iteration\n\n2\n\nCreate the Logistic Regression model and train it on our selected features\n\n3\n\nScore the trained model\n\n4\n\nAdd the features, score, and model as a tuple to the array of our catalogued models\n\n\n\n\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.99609375, LogisticRegression())\n\n\nIt looks like Island, Culmen Length, and Culmen Depth are our most helpful attributes.\nNext, we need to prepare our test data in the same way that we prepared our training data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\nx_test, y_test = prepare_data(test)\n\nWe can then score our trained model on its performance with the test data.\n\nbest_lr.score(x_test[best_attrs], y_test)\n\n1.0\n\n\nGreat! We have 100% accuracy on the testing data. Now for some more plotting…\n\n\nPlotting the Category Regions\nUsing Matplotlib, we can display the categories on a plot as colored regions. This allows us to visualize how the test points are categorized, and we can get a visual representation of the accuracy of the model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are going to look at both the training data and the testing data, split up by the qualitative factor (the islands).\n\nplot_regions(best_lr, x_train[best_attrs], y_train)\nplot_regions(best_lr, x_test[best_attrs], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the model had no trouble at all with the test data–all the points were clearly within their respective categories on the plot. To continue to test our model against unseen data, we can split up the training data to cross-validate. scikit-learn has a function for this, and it will test our Logistic Regression model against different subsets of the training model.\nCross-validating…\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nOur model did very well on the cross-validation in addition to the training data, with an average score of 99.6%. Another way we can visualize its performance is to create a confusion matrix, which shows us specifically what the model predicted in terms of classification. We will do this with our test data.\nConfusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = best_lr.predict(x_test[best_attrs])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nMore intuitive formatting:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nSince the model was 100% correct on the testing data, it is hard to comment on where it could go wrong. However, I predict that false positives for Adelie could occur, since Adelie penguins are found on all three islands. Additionally, the training data included a few penguins who were Chinstraps that were on the border of the Chinstrap and Adelie classifications in the above regions plot.\n\n\nDiscussion\nIn this blog post, we used a Logistic Regression model to classify penguins from the Palmer Penguins dataset. We found that the most important features were the island they were observed on and their culmen dimensions. In general, the model was successful in categorizing the section of the data that we put aside for testing, and it performed well for being trained on only a few features. I am curious if adding more features would improve the accuracy of the model, or if it would introduce overfitting to the model and would become too specific."
  },
  {
    "objectID": "posts/ADS/index.html",
    "href": "posts/ADS/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nIn this blog post, we will walk through the process of training a linear regression model, creating a scoring function, and finding an optimal threshold for a scoring and classification problem. The dataset we are working with is one with data about prospective loan borrowers from a bank. It includes various characteristics of each person and the loan they are requesting. We will use this data to determine a threshold and scoring function to determine whether or not, if given the loan, the borrower is likely to default (or, in laymans terms, violate the terms of the loan by not paying/causing the bank to lose money). We will use the scikit-learn library again for this blog post. First, we will train a logistic regression model. Then we will find optimal values for the weight vector and the threshold so that we can determine the optimal profit/borrower.\n\n\nPart A: Grab the Data\nTo begin, let’s import the data:\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nWe can take a look at the columns so that we know what attributes we are working with.\n\ndf_train.columns\n\nIndex(['person_age', 'person_income', 'person_home_ownership',\n       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n       'loan_int_rate', 'loan_status', 'loan_percent_income',\n       'cb_person_default_on_file', 'cb_person_cred_hist_length'],\n      dtype='object')\n\n\n\n\nPart B: Explore The Data\nNow, we can make a few figures to explore what the data is saying. First, I created a scatterplot to show the relationship between a person’s age and their credit history length. As can be inferred, older people generally have longer credit histories.\n\nfig, ax = plt.subplots(1, 1)\n# cut out the huge outlier--person making 6m/yr--so that we can see patterns more clearly\np1 = sns.scatterplot(df_train[df_train[\"person_age\"] &lt; 100], x = \"cb_person_cred_hist_length\", y = 'person_age', hue = 'cb_person_default_on_file', style = 'cb_person_default_on_file')\n\n\n\n\n\n\n\n\nHowever, I was very interested in the fact that whether or not people defaulted on their loan seemed not to be reliant on age. There are orange x’s in all parts of the plot. Because of this, I wanted to look at the mean age for people who had a history of defaulting and people who didn’t.\n\ndf_train.groupby(\"cb_person_default_on_file\")[\"person_age\"].mean()\n\ncb_person_default_on_file\nN    27.721878\nY    27.793096\nName: person_age, dtype: float64\n\n\nAs you can see, although the average age for people who had defaulted was slightly bigger, they still both had averages of about 27. This was surprising to me because I had previously assumed that people who are older, and therefore have longer credit histories, would be more likely to have defaulted on a loan. I wonder if this is due to financial differences between generations.\nNext, I wanted to look at interest rate based on history of defaulting. I assumed before making the plot that people who had defaulted before would be given a higher interest rate, and I was correct.\n\nbarplot = sns.barplot(df_train, x = \"cb_person_default_on_file\", y = \"loan_int_rate\")\nbarplot.set_title(\"Average loan interest rate separated by previous loan defaults\")\n\nText(0.5, 1.0, 'Average loan interest rate separated by previous loan defaults')\n\n\n\n\n\n\n\n\n\nHere, we can see that the average loan interest rate for those who had not defaulted on a loan previously is about 10%. For those who had, it was about 15%. A 5% increase is quite significant, and it seems like having a clean loan history would significantly benefit someone’s chances at getting a good interest rate.\nNext, I wanted to look at the factors which played into loan intent. To do this, I created a table showing the home ownership counts per loan intent.\n\ndf_train.groupby(\"loan_intent\")[\"person_home_ownership\"].value_counts()\n\nloan_intent        person_home_ownership\nDEBTCONSOLIDATION  RENT                     2260\n                   MORTGAGE                 1841\n                   OWN                        62\n                   OTHER                      15\nEDUCATION          RENT                     2612\n                   MORTGAGE                 2089\n                   OWN                       412\n                   OTHER                      14\nHOMEIMPROVEMENT    MORTGAGE                 1384\n                   RENT                     1252\n                   OWN                       255\n                   OTHER                      11\nMEDICAL            RENT                     2740\n                   MORTGAGE                 1730\n                   OWN                       352\n                   OTHER                      13\nPERSONAL           RENT                     2171\n                   MORTGAGE                 1868\n                   OWN                       354\n                   OTHER                      15\nVENTURE            RENT                     2135\n                   MORTGAGE                 1811\n                   OWN                       648\n                   OTHER                      20\nName: count, dtype: int64\n\n\nWe can see here that there isn’t too much variation, but there are definitely differences that stand out. By far, the most popular type of loan for homeowners is a venture loan, and very few homeowners requested a debt consolidation loan. People with mortgages were spread out pretty evenly across the board, but home improvement had the least mortgagers. By far, home improvement was the least popular type of loan amongst renters. All of these make sense to me for a few reasons. Homeowners gravitated towards venture loans because, in general, they are likely to be more financially stable. Renters did not go for the home improvement loans because they likely don’t have much say in what gets done on the property they are renting. There are so few people in the “other” category that it is hard to pick out trends.\nNext, we are going to prepare the data the same way that we did last time. We will split into x and y, removing the target variables from the training set. We will also get rid of null variables.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nx_train, y_train = prepare_data(df_train)\nx_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n22907 rows × 26 columns\n\n\n\n\n\nPart C: Build a Model\nNow that the data is properly processed, we can train a logistic regression model. This will follow a similar process to the Penguins blog post, where we loop through all possible combinations of 3 features and add their scores and models to a list.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = ['person_age', 'person_income', 'person_emp_length', \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nscores = []\ncombo_array = []\n\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in x_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combo_array.append(cols)\n    LR = LogisticRegression()\n    LR.fit(x_train[cols], y_train)\n    new_score = LR.score(x_train[cols], y_train)\n    scores.append((cols, new_score, LR))\n\nNow, we can find the maximum score in the list of results from the model. We can then split up our results into the attributes, the score, and the logistic regression model.\n\nfrom operator import itemgetter\nbest_combo = max(scores, key = itemgetter(1))\nprint(best_combo)\nbest_attrs = best_combo[0]\nbest_score = best_combo[1]\nbest_lr = best_combo[2]\n\n(['person_emp_length', 'loan_percent_income', 'person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT'], 0.8482559916182826, LogisticRegression())\n\n\nBefore we move on, we should make sure the model doesn’t perform drastically worse on unseen data. Below, we cross-validate our model.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(best_lr, x_train[best_attrs], y_train, cv=5)\nprint(cv_scores_LR.mean())\ncv_scores_LR\n\nWe could have gone through and cross-validated all the possible models, but this score is close enough to our initial score that it is okay to continue.In fact, it performed slightly better on the cross-validation than on the training data. Let’s continue.\nBelow, we can extract the w vector (weights) from our best logistic regression model.\n\nw = pd.Series(best_lr.coef_[0])\nw\n\n0   -0.019247\n1    8.281007\n2   -0.735289\n3   -0.107546\n4   -1.795208\n5    0.265245\ndtype: float64\n\n\n\n\nPart D: Find a Threshold\nNow we begin the process of determining what the most profitable threshold is for the bank. We can start by defining our scoring function, which is just going to be the dot product of the weight vector w.\n\ndef score_function(w, x):\n    return x@w\n\nWe can also plot out these scores by frequency:\n\nx_train['scores'] = score_function(w.values, x_train[best_attrs])\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(x_train['scores'], bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nMost of the scores fall between 0 and 2. This gives us an idea for where the threshold may fall as well.\nWe can now define the benefit function. We are going to stick with the function given in the assignment, because I do not know much about bank profits!\n\ndef get_benefit(loan_amt, loan_int_rate, default):\n    loan_int_rate = loan_int_rate/100\n    if default == False:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**10 - loan_amt\n    else:\n        cost = loan_amt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amt\n    return cost\n\nNext, I added a column into the dataset for c (profit for someone who doesn’t default) and C (profit for someone who defaults). This helps me to be able to filter more easily in the future! I made sure to apply the function to the whole pandas series for each column, since Pandas can handle vector arithmetic.\n\nx_train['c'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], False)\nx_train['C'] = get_benefit(x_train[\"loan_amnt\"], x_train[\"loan_int_rate\"], True)\nx_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nscores\nc\nC\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n1.201226\n4613.567568\n-6997.533847\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n2.404884\n2044.334031\n-6426.108799\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0.640802\n493.650464\n-795.445199\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n1.5449\n4028.690420\n-9390.333437\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n2.297004\n2430.522429\n-3211.752128\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow for the actual calculations! Below, we loop through 101 possible threshold values and find the benefit for each one. We maintain an array of thresholds and an array of benefits for easy plotting.\n\nbest_benefit = 0\nbest_threshold = 0\nt_arr = []\nbenefits = []\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 3, 101): \n    y_pred = x_train['scores'] &gt;= t\n    tn = ((y_pred == 0) & (y_train == 0)).mean()\n    fn = ((y_pred == 0) & (y_train == 1)).mean()\n    benefit = x_train['c'][x_train['scores'] &gt;= t].sum()*tn - x_train['C'][x_train['scores'] &gt;= t].sum()*fn\n    t_arr.append(t)\n    benefits.append(benefit)\n    if benefit &gt; best_benefit: \n        best_benefit = benefit\n        best_threshold = t\n\nplt.plot(t_arr, benefits)\nplt.plot(best_threshold, best_benefit, marker=\"o\")\nplt.title(f\"Best benefit per person: ${(best_benefit/len(x_train[x_train[\"scores\"] &lt; best_threshold])).round(2)} \\nBest threshold: {best_threshold}\")\nplt.xlabel(\"Threshold score\")\nplt.ylabel(\"Total benefit in dollars\")\n\nText(0, 0.5, 'Total benefit in dollars')\n\n\n\n\n\n\n\n\n\nWe have outputted this plot which shows us the curve of threshold plotted against benefit! We can also see that our best benefit per borrower is $1,868.62, and our optimal threshold is 1.14. Now, let’s test this model and threshold against our testing data. Hopefully, since we did well with the cross-validation, we will do well on the yet unseen data.\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\nx_test, y_test = prepare_data(df_test)\nx_test['c'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], False)\nx_test['C'] = get_benefit(x_test[\"loan_amnt\"], x_test[\"loan_int_rate\"], True)\n\n\nt = best_threshold\n\n# compute the scores\nx_test['scores'] = score_function(w.values, x_test[best_attrs])\npreds = x_test['scores'] &gt;= t\n\n# compute error rates\nFN = ((preds == 0) & (y_test == 1)).mean() \nTN = ((preds == 0) & (y_test == 0)).mean() \n# compute the expected gain\ngain = x_test['c'][x_test['scores'] &gt;= t].sum()*TN  - x_test['c'][x_test['scores'] &gt;= t].sum()*FN\ngain/len(x_test[x_test[\"scores\"] &lt; t])\n\nnp.float64(1361.3320954051185)\n\n\nOur profit per borrower is $1,361.33. Not as much as in our training data, but not bad!\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\n\n1. Is it more difficult for people in certain age groups to access credit under your proposed system?\nLet’s look at a plot and find out.\n\nplot = sns.scatterplot(x_train[x_train[\"person_age\"] &lt; 100], x = \"person_age\", y = \"scores\")\nplot.set_title(\"Scores against age for prospective borrowers\")\n\nText(0.5, 1.0, 'Scores against age for prospective borrowers')\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there is much correlation between score and age.\n\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nLet’s check out the data:\n\ndf_train[\"scores\"] = x_train[\"scores\"]\ndf_train.groupby(\"loan_intent\")[\"scores\"].max()\n\nloan_intent\nDEBTCONSOLIDATION     6.14476\nEDUCATION            6.449154\nHOMEIMPROVEMENT       6.06195\nMEDICAL              5.965717\nPERSONAL             6.137947\nVENTURE               5.89633\nName: scores, dtype: object\n\n\nSo the maximum score for medical is lower than almost all the other maximum scores. However, this may not reflect the average score given to medical borrowers.\n\ndf_train.groupby(\"loan_intent\")[\"scores\"].mean()\n\nloan_intent\nDEBTCONSOLIDATION    1.114447\nEDUCATION             1.00498\nHOMEIMPROVEMENT      0.873225\nMEDICAL              1.100077\nPERSONAL             0.969918\nVENTURE              0.900046\nName: scores, dtype: object\n\n\nHere, we can see that the average score for a medical loan request is actually higher than many of the other categories. In general, the Venture category has a low score for both the max score and the mean score. Debt consolidation tends to score hgih, as does education. Medical flips from the high end (mean) to the low end (max). We can also look at the count of each that got a loan, as opposed to the true default values.\n\ndf_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n/var/folders/sy/9tmrg3gx65vf4qjw8jl8ytsc0000gn/T/ipykernel_82223/2869003493.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  df_train[df_train[\"scores\"] &gt;= best_threshold].groupby(\"loan_intent\").size() - df_train[df_train[\"scores\"] &gt;= best_threshold][df_train[\"loan_status\"] == 0].groupby(\"loan_intent\").size()\n\n\nloan_intent\nDEBTCONSOLIDATION    648\nEDUCATION            574\nHOMEIMPROVEMENT      424\nMEDICAL              718\nPERSONAL             534\nVENTURE              468\ndtype: int64\n\n\nHere, we calculated the number of people in each loan intent group that were above the threshold and defaulted. Overall, it seems like the error rate among groups was pretty equal when you consider the size of the groups.\n\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nsns.scatterplot(x = x_train[x_train[\"person_income\"] &lt; 2000000][\"person_income\"], y = x_train[\"scores\"])\n\n\n\n\n\n\n\n\nOverall, it seems like lower-income people are more likely to receive a higher score. This is interesting to me because I would assume those who are higher income would be more likely to pay off their loans. However, maybe interest rates are higher among lower-income people in this dataset. Also, it could have something to do with the reasons people are requesting loans.\n\n\n\nPart G: Write and Reflect\n\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nI think this depends on whether you are thinking from a utilitarian or ethical view of fairness. I also think that fairness is not necessary always ethical in the ways people use it. I believe that it is unfair that people who need help cannot access it, and I think it is sad that profits are often prioritized over people.\nMy definition of fairness: Equity–people who need help get it.\nHowever, I can see how someone would argue that while it is unethical to deny someone a medical loan, it may still be fair. Basing loans off of likelihood of default is an objective way of making the decision, and I think some people see objectivity as fairness.\n\n\n\nDiscussion\nAfter completing this blog post, I had a lot more thoughts about using machine learning to predict human behavior. The first assignment was cool because we could get such high accuracy, but this one (and the in-class examples) made me wonder more about ethics. Predicting human behavior is inherently harder for algorithms, and the consequences of getting it wrong are higher. I generally think that using machine learning in situations like this is tempting, but seems very dangerous. The added probability of being wrong and the added consequences of being wrong generally make for a bad risk management decision. In terms of this case, people who needed money and would have paid on time may get passed over because of an algorithmically generated score."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nThe code for my LR implementation can be found on GitHub here.\nIn this blog post, we will implement Logistic Regression and run various experiments to test our implementation. Logistic Regression uses the gradient of a feature matrix to approximate the best next option for the weight vector w. We iterate through this step until the weight vector is optimal and the loss function has converged. Our initial tests will include a basic test of performance on synthetic data, a comparison of the Gradient Descent Optimizer with and without momentum, and a test in which we force overfitting of the model by increasing the features so they outnumber the number of data points. After these synthetic data experiments, we will run our model on some real-world data to see how it performs on an actual dataset.\n\n\nExperiments\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import GradientDescentOptimizer\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport pandas as pd\nimport numpy as np\n\nFirst, lets define the function with which we will create our synthetic data. The code for this function was taken from Prof. Phil Chodrow’s lecture notes.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\nVanilla Gradient Descent\nNow, we will run our model on this synthetic data with beta=0, which is called Vanilla Gradient Descent. We want to see how it performs without momentum so we can compare later on.\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nfor _ in range(500):\n    loss = opt.loss(X, y)\n    loss_arr.append(loss)\n    opt.step(X, y, alpha = 0.3, beta = 0.0)\n\n\nplt.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"black\")\nplt.title(\"Loss over iterations for Vanilla Gradient Descent\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, we can see that our loss function has converged. Let’s plot our weight vector and see how it separates our data.\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, opt.w)\n\n\n\n\n\n\n\n\nLooking good! Our vector looks visually correct. Now let’s compare with a model where we add momentum.\n\n\nGradient Descent with Momentum\nNow, we want to prove that adding momentum can speed up convergence in some cases. Let’s create a new model with beta = 0.9 and run it for the same number of iterations that we ran our Vanilla Gradient Descent Model for.\n\nopt2 = GradientDescentOptimizer()\nloss_arr2 = []\nfor _ in range(500):\n    loss = opt2.loss(X, y)\n    loss_arr2.append(loss)\n    opt2.step(X, y, alpha = 0.3, beta = 0.9)\n\nNow we can plot the loss curves!\n\nax = plt.axes()\nax.plot(torch.arange(1, len(loss_arr2)+1), loss_arr2, color = \"black\", label = \"GD with Momentum\")\nax.plot(torch.arange(1, len(loss_arr)+1), loss_arr, color = \"blue\", label = \"Vanilla GD\")\nax.set_title(\"Loss for Gradient Descent with Momentum vs Vanilla Gradient Descent\")\nax.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nWe can see that the model with momentum converges much faster than the model without for these choices in alpha and beta.\n\n\nForcing Overfitting\nNow that we’ve confirmed that our model performs as we would expect, let’s test it on some data with too many features. We want to force overfitting in this case.\n\nX_train, y_train = classification_data(n_points=50, noise = 0.5, p_dims=5000)\nX_test, y_test = classification_data(n_points=50, noise = 0.5, p_dims=5000)\n\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nfor _ in range(500):\n    opt.step(X_train, y_train, alpha = 0.3, beta = 0)\n\nWe’ve ran our model, so let’s take a look at the difference between training data accuracy and testing data accuracy.\n\npredicted = opt.predict(X_train)\n(predicted == y_train).sum()/len(predicted)\n\ntensor(1.)\n\n\nWe have 100% accuracy on the training data.\n\npredicted_test = opt.predict(X_test)\n(predicted_test == y_test).sum()/len(predicted_test)\n\ntensor(0.7600)\n\n\nBut only 70% accuracy on the testing data! Because of the amount of features that we put in our dataset, the model tried to hard to be accuracte for all of them, and when we gave it another dataset the accuracy dropped dramatically. Moral of the story: don’t have more features than data points!\n\n\nEmpirical Data\nNow for the fun part! We are going to load in a real-world binary classification dataset and test our model performance on it.\nThe data we will be using is a dataset of breast cancer patients and a control group who had not been diagnosed with breast cancer. It contains features such as age, BMI, and glucose levels. I obtained this data from the UC Irvine Machine Learning Repository, but the data was originally collected and used for a machine learning study by Patricio et al. (2018).\nFirst, let’s process the data and split it into training, testing, and validation datasets.\n\ndf = pd.read_csv(\"./data/anemia_dataset.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nNumber\nName\n%Red Pixel\n%Green pixel\n%Blue pixel\nHb\nAnaemic\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\n\n\n\n\n0\n1\nJafor Alam\n43.2555\n30.8421\n25.9025\n6.3\nYes\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2\nkhadiza\n45.6033\n28.1900\n26.2067\n13.5\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n3\nLalu\n45.0107\n28.9677\n26.0215\n11.7\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n4\nMira nath\n44.5398\n28.9899\n26.4703\n13.5\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n5\nmonoara\n43.2870\n30.6972\n26.0158\n12.4\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe’ll use the features identified by Patricio et al. as the most pertinent features: Age, BMI, Glucose, and Resistin.\n\ndf = df.drop([\"Number\", \"Name\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\"], axis=1)\ndf\n\n\n\n\n\n\n\n\n%Red Pixel\n%Green pixel\n%Blue pixel\nHb\nAnaemic\n\n\n\n\n0\n43.2555\n30.8421\n25.9025\n6.3\nYes\n\n\n1\n45.6033\n28.1900\n26.2067\n13.5\nNo\n\n\n2\n45.0107\n28.9677\n26.0215\n11.7\nNo\n\n\n3\n44.5398\n28.9899\n26.4703\n13.5\nNo\n\n\n4\n43.2870\n30.6972\n26.0158\n12.4\nNo\n\n\n...\n...\n...\n...\n...\n...\n\n\n99\n49.9999\n29.2860\n20.7141\n14.5\nYes\n\n\n100\n42.2324\n30.6757\n27.0919\n6.3\nYes\n\n\n101\n45.6064\n31.9084\n22.4852\n12.7\nNo\n\n\n102\n45.2095\n29.2769\n25.5136\n13.4\nNo\n\n\n103\n43.5706\n29.8094\n26.6199\n12.2\nNo\n\n\n\n\n104 rows × 5 columns\n\n\n\n\nX = df.drop(\"Anaemic\", axis=1)\ny = df[\"Anaemic\"]\ny = y.replace({\"Yes\": 1, \"No\": 0})\n\n/var/folders/sy/9tmrg3gx65vf4qjw8jl8ytsc0000gn/T/ipykernel_78472/975647156.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  y = y.replace({\"Yes\": 1, \"No\": 0})\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=1) # 25% of 80 is 20, so this should create a 60/20/20 split\n\n\nX_train = torch.from_numpy(X_train.to_numpy(dtype = np.float32))\nX_test = torch.from_numpy(X_test.to_numpy(dtype = np.float32))\nX_val = torch.from_numpy(X_val.to_numpy(dtype = np.float32))\ny_train = torch.tensor(y_train.values, dtype=torch.float32)\ny_test = torch.tensor(y_test.values, dtype=torch.float32)\ny_val = torch.tensor(y_val.values, dtype=torch.float32)\n\n\nopt = GradientDescentOptimizer()\nloss_arr = []\nloss_arr_val = []\nfor _ in range(100):\n    loss = opt.loss(X_train, y_train)\n    loss_val = opt.loss(X_val, y_val)\n    loss_arr.append(loss)\n    loss_arr_val.append(loss_val)\n    opt.step(X_train, y_train, alpha = 0.003, beta = 0)\n\n\nloss_arr\n\n[tensor(nan),\n tensor(nan),\n tensor(nan),\n tensor(nan),\n tensor(nan),\n tensor(nan),\n tensor(nan),\n tensor(524.2529),\n tensor(204.7319),\n tensor(67.5799),\n tensor(57.1138),\n tensor(57.1009),\n tensor(55.5209),\n tensor(55.4334),\n tensor(54.5089),\n tensor(54.3825),\n tensor(53.7285),\n tensor(53.5803),\n tensor(53.0678),\n tensor(52.9067),\n tensor(52.4786),\n tensor(52.3098),\n tensor(51.9365),\n tensor(51.7630),\n tensor(51.4275),\n tensor(51.2514),\n tensor(50.9433),\n tensor(50.7658),\n tensor(50.4783),\n tensor(50.3005),\n tensor(50.0290),\n tensor(49.8514),\n tensor(49.5928),\n tensor(49.4158),\n tensor(49.1677),\n tensor(48.9917),\n tensor(48.7524),\n tensor(48.5777),\n tensor(48.3459),\n tensor(48.1727)]\n\n\n\nax = plt.axes()\nax.plot(loss_arr, label=\"Training\")\nax.plot(loss_arr_val, label=\"Validation\")\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\n\nReferences:\nPatrício, M., Pereira, J., Crisóstomo, J. et al. Using Resistin, glucose, age and BMI to predict the presence of breast cancer. BMC Cancer 18, 29 (2018). https://doi.org/10.1186/s12885-017-3877-1"
  },
  {
    "objectID": "posts/newton/index.html",
    "href": "posts/newton/index.html",
    "title": "Newtons Method and Adam Optimizers",
    "section": "",
    "text": "Abstract\nIn this blog post, we will implement two advanced optimization techniques and compare them. First, we will implement a Newton’s Method Optimizer, which uses the Hessian matrix of the data as well as the gradient to update the weight vector. This method can converge very quickly, but it is sensitive to saddle points and struggles with certain datasets. Next, we will implement the Adam Optimizer, which operates on a subset of the data, and therefore can complete updates much more quickly. Because of this, however, the Adam Optimizer needs many more steps to converge. Implementations of these algorithms are located on my Github here. This blog post will aim to compare Newton and Adam and determine how much more efficient Adam can be. Do the faster steps fully offset the need for many more iterations?\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport math\nimport time\n\n\n\nPreliminary Testing of Newton’s Method\nFirst, we will take a synthetic dataset and test Newton’s Method. This will hopefully eliminate any errors that could occur because of the data, and give us a good indication that our implementation works.\n\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X.double(), y.double()\n\nX, y = classification_data(noise = 0.5)\n\nWe will create two models: one that uses Newton’s Method as the optimizer and one that uses the Gradient Descent Optimizer. We will then compare and check to see that the w vectors they create are approximately similar.\n\nnewton = NewtonOptimizer()\nloss_arr = []\nloss = torch.inf\nwhile loss &gt;= 64.066:\n    loss = newton.loss(X, y)\n    loss_arr.append(loss)\n    newton.step(X, y, alpha = 0.1)\n\n\nGD = GradientDescentOptimizer()\nloss_arr2 = []\nloss = torch.inf\nwhile loss &gt;= 64.066:\n    loss = GD.loss(X, y)\n    loss_arr2.append(loss)\n    GD.step(X, y, alpha = 0.3, beta = 0)\n\n\nax = plt.axes()\nax.plot(loss_arr)\nax.plot(loss_arr2)\n\n\n\n\n\n\n\n\n\nplt.plot(loss_arr2)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nThey both have converged, so let’s plot the w vectors and make sure they are approximately similar.\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, newton.w)\n\n\n\n\n\n\n\n\n\ndef plot_data_and_w(X, y, w):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Label 0')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Label 1')\n\n    # Decision boundary: w0*x + w1*y + w2 = 0\n    x_vals = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n    if w[1] != 0:\n        y_vals = -(w[0] * x_vals + w[2]) / w[1]\n        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_data_and_w(X, y, GD.w)\n\n\n\n\n\n\n\n\n\nprint(newton.w, GD.w)\n\ntensor([ 3.5198,  2.5732, -2.6125], dtype=torch.float64) tensor([ 3.3234,  2.5390, -2.5633], dtype=torch.float64)\n\n\nThe w vectors are almost exactly the same! Newton converged with many fewer iterations than Gradient Descent.\n\n\nNewton’s Method Experiments\nIt took me a long time to find a dataset that worked well for Newton’s method. Since the step function finds the inverse of the Hessian Matrix, I was running into issues where the Hessian was singular and therefore could not be inverted. I therefore switched to using torch.pinverse so that the loop would continue to run. This did result in some NaN values in the loss array.\nThe data we’ll be using is data on Titanic passengers and whether or not they survived. This dataset was obtained from Kaggle.\nFirst, I read in the data and did some visualizations to better understand the features included.\n\ndf = pd.read_csv(\"/Users/ellisterling/Documents/spring25/csci0451/ellisterling.github.io/posts/newton/data/Titanic-Dataset.csv\")\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\nHere I chose a subset of the features based on both plotting and running the algorithm to see what affected accuracy.\n\nfilter = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Survived\"]\n\nOne such plot was a count plot of the class that the passengers were in, with separate bars for those who survived and those who didn’t. We can see here that there were many more passengers in third class. First class was most likely to survive, second class was about half and half, and the vast majority of people in third class passed away.\n\nax = sns.countplot(df, x = \"Pclass\", hue = \"Survived\")\nax.set_title(\"Survival by Class\")\n\nText(0.5, 1.0, 'Survival by Class')\n\n\n\n\n\n\n\n\n\nWe can also check the number of people who survived versus didn’t. Below, we can see that less than half the passengers survived.\n\ndf.groupby(\"Survived\")[\"Survived\"].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\nNow we are going to process the data to get it into tensor form so the Newton and Adam Optimizers can hande it.\n\ndf = df[filter]\ndf = df.dropna()\ndf = pd.get_dummies(df)\n\nTrain test split!\n\ndf_train, df_test = train_test_split(df, test_size = 0.3)\nX_train = df_train.drop(\"Survived\", axis=1)\ny_train = df_train[\"Survived\"]\nX_test = df_test.drop(\"Survived\", axis=1)\ny_test = df_test[\"Survived\"]\n\n\nX_train_tensor = torch.from_numpy(X_train.to_numpy().astype(float))\nX_test_tensor = torch.from_numpy(X_test.to_numpy().astype(float))\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n\n\nX_train_tensor = X_train_tensor.double()\ny_train_tensor = y_train_tensor.double()\nX_test_tensor = X_test_tensor.double()\ny_test_tensor = y_test_tensor.double()\n\nNow we can run experiments on this data with the Newton Optimizer.\n\nw_dims = X_train_tensor.size()[1]\n\n\nnewton = NewtonOptimizer()\nloss_arr3 = []\n\n\nnewton_start_time = time.time()\nloss = math.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = newton.loss(X_train_tensor, y_train_tensor)\n    loss_arr3.append(loss)\n    newton.step(X_train_tensor, y_train_tensor, alpha = 0.01)\nnewton_end_time = time.time()\n\n\nplt.plot(torch.arange(1, len(loss_arr3)+1), loss_arr3, color = \"black\")\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Newton Optimizer Loss Convergence\")\n\nText(0.5, 1.0, 'Newton Optimizer Loss Convergence')\n\n\n\n\n\n\n\n\n\nInspect the weight vector:\n\nnewton.w\n\ntensor([-8.4734e-01, -1.0634e-02,  2.8755e-03,  3.2690e+00,  1.4170e+00],\n       dtype=torch.float64)\n\n\n\ny_test_tensor\n\ntensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n        0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n        1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n       dtype=torch.float64)\n\n\n\npredicted = newton.predict(X_test_tensor)\npredicted\n\ntensor([1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n        1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n        1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n        0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,\n        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.])\n\n\nLet’s check the accuracy:\n\n#find score\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.7349)\n\n\nPretty good! It would be hard to visualize a 5d vector like this on a plot as we did with the synthetic data, but the score on our data is pretty good. Let’s look at the confusion matrix too:\n\n#find confusion matrix\nfrom sklearn.metrics import confusion_matrix\nC = confusion_matrix(y_test_tensor, predicted)\nC\n\narray([[89, 38],\n       [19, 69]])\n\n\nWe have a higher error rate for passengers who survived. Now, let’s make an example where alpha is too large and Newton’s method fails to converge.\n\ngd = GradientDescentOptimizer()\ngd_loss = []\nt = 1\nloss = torch.inf\nbatch_size = 10\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = gd.loss(X_train_tensor, y_train_tensor)\n    gd_loss.append(loss)\n    gd.step(X_train_tensor, y_train_tensor, 0.0001, 0.9)\n\n\nnewton = NewtonOptimizer()\nn_loss = []\nfor t in range(10000):\n    loss = newton.loss(X_train_tensor, y_train_tensor)\n    n_loss.append(loss)\n    newton.step(X_train_tensor, y_train_tensor, alpha = 5)\n\n\nplt.plot(n_loss)\n\n\n\n\n\n\n\n\nWe can see from the plot that the model didn’t converge, and does not look to be on the course to converge.\n\npredicted = newton.predict(X_test_tensor)\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.5953)\n\n\nWe can also see that it didn’t reach the level of accuracy it was reaching before.\n\n\nAdam Optimizer Experiments\n\nadam = AdamOptimizer()\nadam_loss = []\n\n\nadam_start_time = time.time()\nt = 1\nloss = torch.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = adam.loss(X_train_tensor, y_train_tensor)\n    adam_loss.append(loss)\n    adam.step(X_train_tensor, y_train_tensor, 10, 0.01, 0.9, 0.999, t)\n    t += 1\nadam_end_time = time.time()\n\n\nplt.plot(adam_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Adam Optimizer Loss Convergence\")\n\nText(0.5, 1.0, 'Adam Optimizer Loss Convergence')\n\n\n\n\n\n\n\n\n\n\npredicted = adam.predict(X_test_tensor)\n(predicted == y_test_tensor).sum()/len(predicted)\n\ntensor(0.7442)\n\n\nGreat! We are able to obtain similar accuracy by predicting with Adam to Newton. Now, let’s compare against standard minibatch stochastic gradient descent. I did this by selecting a batch in the loop, and using my standard gradient descent model from the Logistic Regression blog post.\n\nsgd = GradientDescentOptimizer()\nsgd_loss = []\nt = 1\nloss = torch.inf\nbatch_size = 10\nfor t in range(20000):\n    i = torch.randint(0, len(y_train_tensor), (batch_size,))\n    loss = sgd.loss(X_train_tensor, y_train_tensor)\n    sgd_loss.append(loss)\n    sgd.step(X_train_tensor[i], y_train_tensor[i], 0.01, 0.9)\n\nWe ran it for a minute and still it never reached a loss of 240. Let’s take a look at the loss plot:\n\nplt.plot(sgd_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for SGD with alpha = .01\")\n\nText(0.5, 1.0, 'Convergence for SGD with alpha = .01')\n\n\n\n\n\n\n\n\n\nClearly, at this step size (.01), SGD does not converge. Let’s try with some smaller timesteps.\n\nadam = AdamOptimizer()\nadam_loss = []\n\n\nt = 1\nloss = torch.inf\nwhile loss &gt;= 240 or torch.isnan(loss):\n    loss = adam.loss(X_train_tensor, y_train_tensor)\n    adam_loss.append(loss)\n    adam.step(X_train_tensor, y_train_tensor, 10, 0.0001, 0.9, 0.999, t)\n    t += 1\n\n\nplt.plot(adam_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for Adam with alpha = 0.0001\")\n\nText(0.5, 1.0, 'Convergence for Adam with alpha = 0.0001')\n\n\n\n\n\n\n\n\n\n\nsgd = GradientDescentOptimizer()\nsgd_loss = []\nt = 1\nloss = torch.inf\nbatch_size = 10\nwhile loss &gt;= 240 or torch.isnan(loss):\n    i = torch.randint(0, len(y_train_tensor), (batch_size,))\n    loss = sgd.loss(X_train_tensor, y_train_tensor)\n    sgd_loss.append(loss)\n    sgd.step(X_train_tensor[i], y_train_tensor[i], 0.0001, 0.9)\n\n\nplt.plot(sgd_loss)\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence for SGD with alpha = 0.0001\")\n\nText(0.5, 1.0, 'Convergence for SGD with alpha = 0.0001')\n\n\n\n\n\n\n\n\n\nThe plot looks a little weirder than the Adam plot, but it certainly converges. It also completed its training loop in 1.2 seconds, whereas Adam completed its training loop in 2.3 seconds.\nSGD performs faster with smaller steps, but fails to converge at larger steps. Adam converges more reliably, but takes a longer time with smaller step sizes (e.g. .0001).\n\n\nNewton and Adam Comparison\nIn the sections above, we ran Newton and Adam optimizers until the loss reached 240, which seemed to be about where they converged for this dataset. Adam took more iterations, but frequently beat out Newton in terms of time taken to run to convergence. On occasion, Newton would beat Adam, but this likely depended on the train-test split of the data and the initial choice of the weight vector w for both optimizers. When run with a step size of .01, these were their times:\n\nadam_time = adam_end_time - adam_start_time\nnewton_time = newton_end_time - newton_start_time\nprint(f\"Newton Time: {newton_time}\\nAdam Time: {adam_time}\")\n\nNewton Time: 0.06209516525268555\nAdam Time: 0.045562028884887695\n\n\nGenerally, it seems that Adam is faster, even though it takes more iterations to reach convergence. However, depending on the train-test split and the initial choice of w, Newton may perform better. Both Adam and Newton perform best with small step sizes.\n\n\nDiscussion\nOverall, running these experiments showed me that the optimizer that performs the best is heavily dependent on the dataset and the step size. One thing that interested me about this is how increasing complexity of the optimizer/implementing a more advanced method means that, in some ways, we are sacrificing reliability of convergence. However, these more advanced methods can handle more complex datasets than classic gradient descent. I was also interested that the train-test split and the choice of initial w vectors drastically affected the time and iterations that it took Newton and Adam to converge. Overall, it seemed that Adam was faster, but Newton could overtake it a lot of the time. I would be interested to try these optimizers on a bunch of datasets to get better context for which one has overall better performance."
  },
  {
    "objectID": "posts/quantifying-bias/index.html",
    "href": "posts/quantifying-bias/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "Narayanan’s Position\nAs a quantitative researcher, Narayanan acknowledges in his speech that, although he is pointing out the flaws of quantitative methodology, he recognizes its importance in the world of research. Throughout his speech, he highlights several flaws in quantitating discrimination, most of which boil down to the fact that quantitative data does not tell the whole story. His perspective is that quanititative methods are just one piece of the puzzle. They are necessary, but they are not the only way to gain knowledge. Quantitative ways of measuring bias should be used in conjunction with other forms of measurement and auditing to ensure that all perspectives are observed and the fullest picture of the situation is found (Narayanan (2022)).\n\n\nBenefits of Quantifying Bias\nIn Narayanan (2022), he discusses how the ProPublica investigation into the COMPAS algorithm was successful in part because there as a quantifiable result that could “definitively” say whether or not the COMPAS algorithm was biased. Evidently, this succeeded, in part because they were able to quantify ways in which they could see evidence of bias. It then gained a lot of attention because of its success. Narayanan posits that if they had been searching in a more qualitative way, they would have gained less attention from their study and therefore would have reached a smaller audience. Essentially, humans are more likely to believe something if there is evidence for it, and quantitative evidence is sometimes prioritized over qualitative evidence (especially in the scientific community).\n\n\nDrawbacks of Quantifying Bias\nAs outlined by Narayanan:\n• Use of snapshot datasets\n• Use of data provided by company\n• Explaining away discrimination\n• Wrong locus of intervention\n• Objectivity illusion\n• Performativity\nNarayanan outlines several potential pitfalls of quantifying bias in his speech (Narayanan (2022)). The first one he outlines is the choice of null hypothesis. What reserachers choose as their “null hypothesis”, or base assumption of neutrality, drastically affects whether or not we call something biased. Generally, researchers say that the null hypothesis is the absence of discrimination. This sets up studies to be biased from the start, since assuming that the base state is a lack of bias ignores the systemic bias that is present in many aspects of society. Narayanan brings up the example of diversity initiatives in schools and other programs–they focus on fixing the surface level problem without addressing the root cause (discrimination) because the base assumption is that academia is an unbiased field to begin with. He also discusses the bias that is inherently present in data, even more so when the data is provided by an institution with stakes in the investigation. This reminded me of how some fossil fuel companies have funded climate science so that they could control what results came out of the studies.\n\n\nDiscussion and Synthesis\nI generally agree with Narayanan’s perspectives on quantitative methods. I think that they have their place, but also I think what is needed is a perspective shift–that quantitative methods are not always better/more informative than qualitative ones, and that we don’t have to be able to simulate everything perfectly mathematically. While I recognize the benefits of quantitativeness, I also believe that quantifying everything we see will usually create more problems than not.\n\n\n\n\n\n\n\n\nReferences\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elli Sterling’s CSCI 0451 Blog",
    "section": "",
    "text": "Newtons Method and Adam Optimizers\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing bias with machine learning models.\n\n\n\n\n\nMar 8, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nDesigning an algorithm to decide whether or not loan applicants should receive loans.\n\n\n\n\n\nFeb 27, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying penguins by species based on the Palmer Penguins dataset\n\n\n\n\n\nFeb 12, 2025\n\n\nEleanor Sterling\n\n\n\n\n\n\nNo matching items"
  }
]